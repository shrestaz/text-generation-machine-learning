{"cells":[{"metadata":{"id":"80xmUzUJgGWZ","colab_type":"code","trusted":true,"colab":{}},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\ntf.enable_eager_execution()\n\nimport numpy as np\nimport os\nimport time","execution_count":0,"outputs":[]},{"metadata":{"id":"efHQ8RUcwqAi","colab_type":"code","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"11f311e1-e9fe-460e-8354-25d64a9a06d7","executionInfo":{"status":"ok","timestamp":1556287550867,"user_tz":-120,"elapsed":1862,"user":{"displayName":"Jannik Verdoner","photoUrl":"","userId":"08021834613783853906"}}},"cell_type":"code","source":"# Creating and converting dataset to tensorflow\npath = tf.keras.utils.get_file('dracula.txt', 'http://www.gutenberg.org/cache/epub/345/pg345.txt')\ntext = open(path, 'rb').read().decode(encoding='utf-8')\nvocabulary = sorted(set(text))\nvocabulary_size = len(vocabulary)","execution_count":null,"outputs":[]},{"metadata":{"id":"cbCv2ccgwknz","colab_type":"code","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"919f5af1-6f73-4dce-bcd1-63a08d989cc5","executionInfo":{"status":"ok","timestamp":1556287552980,"user_tz":-120,"elapsed":3963,"user":{"displayName":"Jannik Verdoner","photoUrl":"","userId":"08021834613783853906"}}},"cell_type":"code","source":"# map unique characters to indices and vice versa\ncharMapping = {u:i for i, u in enumerate(vocabulary)}\nindiceMapping = np.array(vocabulary)\ntext_as_int = np.array([charMapping[c] for c in text])\n\nsequence_length = 100\nexamples_per_epoch = len(text)//sequence_length # floor division\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nsequences = char_dataset.batch(sequence_length+1, drop_remainder=True)\n\nnum_of_characters = len(text)\nnum_of_vocabulary = len(vocabulary)\nprint(\"Total Characters: \", num_of_characters)\nprint(\"Total Vocabulary: \", num_of_vocabulary)\n\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","execution_count":null,"outputs":[]},{"metadata":{"id":"4gpz5u3W32hK","colab_type":"code","trusted":true,"colab":{}},"cell_type":"code","source":"BATCH_SIZE = 512\nsteps_per_epoch = examples_per_epoch//BATCH_SIZE # floor division\nBUFFER_SIZE = 10000\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)","execution_count":0,"outputs":[]},{"metadata":{"id":"sj5x42V22Y3U","colab_type":"code","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"54384b6f-8d73-4ed4-ebac-c5a2ba5d927c","executionInfo":{"status":"ok","timestamp":1556287553145,"user_tz":-120,"elapsed":4113,"user":{"displayName":"Jannik Verdoner","photoUrl":"","userId":"08021834613783853906"}}},"cell_type":"code","source":"# Build model\nembedding_dimension = 256\nrnn_units = 1024\n\nlstm = tf.keras.layers.CuDNNLSTM # fast LSTM implementation backed by cuDNN\n\ndef build_model(vocab_size, embedding_dim, units, batch_size, drop_out):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]), # Input layer\n    lstm(rnn_units, # Hidden layer 1\n        return_sequences=True, \n        recurrent_initializer='lecun_uniform',\n        stateful=True),\n    tf.keras.layers.Dropout(drop_out),\n    lstm(rnn_units, # Hidden layer 2\n        return_sequences=True, \n        recurrent_initializer='lecun_uniform',\n        stateful=True),\n    tf.keras.layers.Dropout(drop_out),\n    tf.keras.layers.Dense(vocab_size) # Output layer\n  ])\n  return model\n\nmodel = build_model(\n  vocab_size = vocabulary_size, \n  embedding_dim=embedding_dimension, \n  units=rnn_units, \n  batch_size=BATCH_SIZE,\n  drop_out=0.2)","execution_count":null,"outputs":[]},{"metadata":{"id":"L1A0cMnYh_RK","colab_type":"code","trusted":true,"colab":{}},"cell_type":"code","source":"# Validate the model\nfor input_example_batch, target_example_batch in dataset.take(1): \n  example_batch_predictions = model(input_example_batch)\n  \nsampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","execution_count":0,"outputs":[]},{"metadata":{"id":"hsQZCK_-42_R","colab_type":"code","trusted":true,"outputId":"0c21e630-0c2f-4076-a0a3-79585e771c00","colab":{"base_uri":"https://localhost:8080/","height":13722},"executionInfo":{"status":"ok","timestamp":1556296680238,"user_tz":-120,"elapsed":1911388,"user":{"displayName":"Jannik Verdoner","photoUrl":"","userId":"08021834613783853906"}}},"cell_type":"code","source":"# Train the model\ndef loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n\nmodel.compile(tf.train.AdamOptimizer(), loss=loss)\n\n# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)\n\nEPOCHS=400\nhistory = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])","execution_count":null,"outputs":[]},{"metadata":{"id":"vrPopEcKfTbv","colab_type":"code","trusted":true,"colab":{}},"cell_type":"code","source":"tf.train.latest_checkpoint(checkpoint_dir)\nmodel = build_model(vocabulary_size, embedding_dimension, rnn_units, batch_size=1, drop_out=0.2)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","execution_count":0,"outputs":[]},{"metadata":{"id":"m_Evgr4Pwydc","colab_type":"code","trusted":true,"colab":{}},"cell_type":"code","source":"# Generate text algorithm\ndef generate_text(model, num_char_to_generate, temperature, start_string):\n  # Converting our start string to numbers (vectorizing) \n  input_eval = [charMapping[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_char_to_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n\n      # using a multinomial distribution to predict the word returned by the model\n      predictions = predictions / temperature\n      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n      \n      # We pass the predicted word as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n      \n      text_generated.append(indiceMapping[predicted_id])\n\n  return (start_string + ''.join(text_generated))","execution_count":0,"outputs":[]},{"metadata":{"id":"n-jikOVZ710c","colab_type":"code","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1360},"outputId":"48415129-da35-4749-a3ed-86633cb395d3","executionInfo":{"status":"ok","timestamp":1556296722169,"user_tz":-120,"elapsed":40685,"user":{"displayName":"Jannik Verdoner","photoUrl":"","userId":"08021834613783853906"}}},"cell_type":"code","source":"  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n\nprint(generate_text(model, 5000, .6, \"Dracula moved to Copenhagen because \"))","execution_count":null,"outputs":[]},{"metadata":{"id":"9q6NKUhHMERD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"ceeeddfa-333f-4297-d7ba-3e7586635b85","executionInfo":{"status":"ok","timestamp":1556296722454,"user_tz":-120,"elapsed":288,"user":{"displayName":"Jannik Verdoner","photoUrl":"","userId":"08021834613783853906"}},"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nloss = history.history['loss']\nnewLoss = np.squeeze(loss)\nplt.figure(figsize=(10,10))\nplt.plot(newLoss)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Text_Generation_2_hidden_layers_lstm.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}