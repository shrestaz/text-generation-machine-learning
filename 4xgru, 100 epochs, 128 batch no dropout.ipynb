{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80xmUzUJgGWZ"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efHQ8RUcwqAi"
   },
   "outputs": [],
   "source": [
    "# Creating and converting dataset to tensorflow\n",
    "path = tf.keras.utils.get_file('dracula.txt', 'http://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
    "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "vocabulary = sorted(set(text))\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "cbCv2ccgwknz",
    "outputId": "5d528eba-3680-4eba-af2d-8ee5fe15dc58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  883114\n",
      "Total Vocabulary:  102\n"
     ]
    }
   ],
   "source": [
    "# map unique characters to indices and vice versa\n",
    "charMapping = {u:i for i, u in enumerate(vocabulary)}\n",
    "indiceMapping = np.array(vocabulary)\n",
    "text_as_int = np.array([charMapping[c] for c in text])\n",
    "\n",
    "sequence_length = 100\n",
    "examples_per_epoch = len(text)//sequence_length # floor division\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "\n",
    "num_of_characters = len(text)\n",
    "num_of_vocabulary = len(vocabulary)\n",
    "print(\"Total Characters: \", num_of_characters)\n",
    "print(\"Total Vocabulary: \", num_of_vocabulary)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gpz5u3W32hK"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE # floor division\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sj5x42V22Y3U"
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "embedding_dimension = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "lstm = tf.keras.layers.CuDNNLSTM # fast LSTM implementation backed by cuDNN\n",
    "gru = tf.keras.layers.CuDNNGRU # fast LSTM implementation backed by cuDNN\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, units, batch_size, drop_out):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]), # Input layer\n",
    "    gru(rnn_units, # Hidden layer 1\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='lecun_uniform',\n",
    "        stateful=True),\n",
    "    gru(rnn_units, # Hidden layer 2\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='lecun_uniform',\n",
    "        stateful=True),\n",
    "    gru(rnn_units, # Hidden layer 3\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='lecun_uniform',\n",
    "        stateful=True),\n",
    "    gru(rnn_units, # Hidden layer 4\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='lecun_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size) # Output layer\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = vocabulary_size, \n",
    "  embedding_dim=embedding_dimension, \n",
    "  units=rnn_units, \n",
    "  batch_size=BATCH_SIZE,\n",
    "  drop_out=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1A0cMnYh_RK"
   },
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  \n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3592
    },
    "colab_type": "code",
    "id": "hsQZCK_-42_R",
    "outputId": "4c01c2fc-d23b-440a-c6bd-3bdef0f5e1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 100, 102)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.62594\n",
      "Epoch 1/100\n",
      "68/68 [==============================] - 36s 535ms/step - loss: 3.2739\n",
      "Epoch 2/100\n",
      "68/68 [==============================] - 37s 546ms/step - loss: 2.3937\n",
      "Epoch 3/100\n",
      "68/68 [==============================] - 39s 566ms/step - loss: 1.9828\n",
      "Epoch 4/100\n",
      "68/68 [==============================] - 39s 566ms/step - loss: 1.7173\n",
      "Epoch 5/100\n",
      "68/68 [==============================] - 38s 564ms/step - loss: 1.5497\n",
      "Epoch 6/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 1.4416\n",
      "Epoch 7/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 1.3676\n",
      "Epoch 8/100\n",
      "68/68 [==============================] - 38s 565ms/step - loss: 1.3125\n",
      "Epoch 9/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 1.2663\n",
      "Epoch 10/100\n",
      "68/68 [==============================] - 38s 564ms/step - loss: 1.2279\n",
      "Epoch 11/100\n",
      "68/68 [==============================] - 39s 571ms/step - loss: 1.1864\n",
      "Epoch 12/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 1.1533\n",
      "Epoch 13/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 1.1167\n",
      "Epoch 14/100\n",
      "68/68 [==============================] - 38s 566ms/step - loss: 1.0810\n",
      "Epoch 15/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 1.0437\n",
      "Epoch 16/100\n",
      "68/68 [==============================] - 39s 566ms/step - loss: 1.0044\n",
      "Epoch 17/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.9663\n",
      "Epoch 18/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.9252\n",
      "Epoch 19/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.8817\n",
      "Epoch 20/100\n",
      "68/68 [==============================] - 38s 566ms/step - loss: 0.8401\n",
      "Epoch 21/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.7958\n",
      "Epoch 22/100\n",
      "68/68 [==============================] - 38s 565ms/step - loss: 0.7504\n",
      "Epoch 23/100\n",
      "68/68 [==============================] - 38s 565ms/step - loss: 0.7060\n",
      "Epoch 24/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.6651\n",
      "Epoch 25/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.6239\n",
      "Epoch 26/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.5879\n",
      "Epoch 27/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.5513\n",
      "Epoch 28/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.5192\n",
      "Epoch 29/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.4889\n",
      "Epoch 30/100\n",
      "68/68 [==============================] - 38s 566ms/step - loss: 0.4646\n",
      "Epoch 31/100\n",
      "68/68 [==============================] - 38s 565ms/step - loss: 0.4421\n",
      "Epoch 32/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.4207\n",
      "Epoch 33/100\n",
      "68/68 [==============================] - 38s 563ms/step - loss: 0.4029\n",
      "Epoch 34/100\n",
      "68/68 [==============================] - 39s 571ms/step - loss: 0.3885\n",
      "Epoch 35/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.3754\n",
      "Epoch 36/100\n",
      "68/68 [==============================] - 39s 571ms/step - loss: 0.3628\n",
      "Epoch 37/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.3532\n",
      "Epoch 38/100\n",
      "68/68 [==============================] - 39s 571ms/step - loss: 0.3444\n",
      "Epoch 39/100\n",
      "68/68 [==============================] - 38s 563ms/step - loss: 0.3369\n",
      "Epoch 40/100\n",
      "68/68 [==============================] - 39s 566ms/step - loss: 0.3304\n",
      "Epoch 41/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.3238\n",
      "Epoch 42/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.3169\n",
      "Epoch 43/100\n",
      "68/68 [==============================] - 38s 565ms/step - loss: 0.3133\n",
      "Epoch 44/100\n",
      "68/68 [==============================] - 39s 570ms/step - loss: 0.3081\n",
      "Epoch 45/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.3032\n",
      "Epoch 46/100\n",
      "68/68 [==============================] - 38s 566ms/step - loss: 0.3044\n",
      "Epoch 47/100\n",
      "68/68 [==============================] - 38s 564ms/step - loss: 0.3025\n",
      "Epoch 48/100\n",
      "68/68 [==============================] - 38s 565ms/step - loss: 0.2974\n",
      "Epoch 49/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2940\n",
      "Epoch 50/100\n",
      "68/68 [==============================] - 38s 565ms/step - loss: 0.2932\n",
      "Epoch 51/100\n",
      "68/68 [==============================] - 38s 564ms/step - loss: 0.2937\n",
      "Epoch 52/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2914\n",
      "Epoch 53/100\n",
      "68/68 [==============================] - 38s 564ms/step - loss: 0.2911\n",
      "Epoch 54/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.2863\n",
      "Epoch 55/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2864\n",
      "Epoch 56/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.2819\n",
      "Epoch 57/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.2834\n",
      "Epoch 58/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2819\n",
      "Epoch 59/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.2819\n",
      "Epoch 60/100\n",
      "68/68 [==============================] - 38s 564ms/step - loss: 0.2786\n",
      "Epoch 61/100\n",
      "68/68 [==============================] - 38s 565ms/step - loss: 0.2765\n",
      "Epoch 62/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.2750\n",
      "Epoch 63/100\n",
      "68/68 [==============================] - 38s 561ms/step - loss: 0.2754\n",
      "Epoch 64/100\n",
      "68/68 [==============================] - 38s 564ms/step - loss: 0.2763\n",
      "Epoch 65/100\n",
      "68/68 [==============================] - 38s 566ms/step - loss: 0.2771\n",
      "Epoch 66/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.2771\n",
      "Epoch 67/100\n",
      "68/68 [==============================] - 39s 572ms/step - loss: 0.2754\n",
      "Epoch 68/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2755\n",
      "Epoch 69/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.2744\n",
      "Epoch 70/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2747\n",
      "Epoch 71/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2737\n",
      "Epoch 72/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2727\n",
      "Epoch 73/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2758\n",
      "Epoch 74/100\n",
      "68/68 [==============================] - 38s 566ms/step - loss: 0.2745\n",
      "Epoch 75/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.2757\n",
      "Epoch 76/100\n",
      "68/68 [==============================] - 39s 566ms/step - loss: 0.2760\n",
      "Epoch 77/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2774\n",
      "Epoch 78/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.2742\n",
      "Epoch 79/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.2737\n",
      "Epoch 80/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.2723\n",
      "Epoch 81/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.2751\n",
      "Epoch 82/100\n",
      "68/68 [==============================] - 39s 569ms/step - loss: 0.2721\n",
      "Epoch 83/100\n",
      "68/68 [==============================] - 39s 572ms/step - loss: 0.2751\n",
      "Epoch 84/100\n",
      "68/68 [==============================] - 39s 570ms/step - loss: 0.2757\n",
      "Epoch 85/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2749\n",
      "Epoch 86/100\n",
      "68/68 [==============================] - 39s 575ms/step - loss: 0.2749\n",
      "Epoch 87/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.2772\n",
      "Epoch 88/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.2763\n",
      "Epoch 89/100\n",
      "68/68 [==============================] - 39s 567ms/step - loss: 0.2753\n",
      "Epoch 90/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2716\n",
      "Epoch 91/100\n",
      "68/68 [==============================] - 39s 571ms/step - loss: 0.2707\n",
      "Epoch 92/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2729\n",
      "Epoch 93/100\n",
      "68/68 [==============================] - 39s 566ms/step - loss: 0.2709\n",
      "Epoch 94/100\n",
      "68/68 [==============================] - 39s 566ms/step - loss: 0.2728\n",
      "Epoch 95/100\n",
      "68/68 [==============================] - 38s 562ms/step - loss: 0.2726\n",
      "Epoch 96/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2739\n",
      "Epoch 97/100\n",
      "68/68 [==============================] - 39s 568ms/step - loss: 0.2765\n",
      "Epoch 98/100\n",
      "68/68 [==============================] - 38s 566ms/step - loss: 0.2777\n",
      "Epoch 99/100\n",
      "68/68 [==============================] - 39s 570ms/step - loss: 0.2778\n",
      "Epoch 100/100\n",
      "68/68 [==============================] - 39s 570ms/step - loss: 0.2792\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
    "\n",
    "model.compile(tf.train.AdamOptimizer(), loss=loss)\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "EPOCHS=100\n",
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrPopEcKfTbv"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocabulary_size, embedding_dimension, rnn_units, batch_size=1, drop_out=0.2)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_Evgr4Pwydc"
   },
   "outputs": [],
   "source": [
    "# Generate text algorithm\n",
    "def generate_text(model, num_char_to_generate, temperature, start_string):\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [charMapping[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_char_to_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(indiceMapping[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1515
    },
    "colab_type": "code",
    "id": "n-jikOVZ710c",
    "outputId": "74231196-fcf7-4085-a176-4d2fb03c80fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dracula moved to Copenhagen because in order that he has placed the floor. The instant he seemed as though he would open the door 'isself an' 'elped me,\r\n",
      "and r that a sudden glimpse of the angle of the building, and the light fell\r\n",
      "on the window. There distinctly was Lucy with her now how to conter last horribly weak. My face is being\r\n",
      "soall, and had to go round headstones and railed---\r\n",
      "\r\n",
      "CHAPTER X\r\n",
      "\r\n",
      "Jressed for his answer. I was afraid to speak or move. The time is\r\n",
      "still some day and nights principles of him which we are on fine and worn out what it is possed\r\n",
      "into the room where was no mapit once, and he must have sunk down but a constant\r\n",
      "sufferens from a road and say:--\r\n",
      "\r\n",
      "\"This is the way.\"\r\n",
      "\r\n",
      "\"How know you it?\" I ask.\r\n",
      "\r\n",
      "\"Of course I know you praise from your letters to Mina, for\r\n",
      "to her I could speak for quite like to oblige him to east and softly. She has got the child and said:--\r\n",
      "\r\n",
      "\"Brave lad! A moment's courage, and returned with the decanter.\r\n",
      "He wetted the pointer to the port and saw that he took out a tiny face in the\r\n",
      "pung as to his brain for a time the words, but yet I know not how to speak them.\"\r\n",
      "\r\n",
      "\"Indeed,\" I said, \"you speak excellently.\"\r\n",
      "\r\n",
      "\"But I must try to get a few hours I husband whose blood.\"\r\n",
      "\r\n",
      "\"And now I pray you, if I see you, saw journal when you trouble that you have given me to go in the draught of the\r\n",
      "open door. The old man motioned me how I had come to him. The captain, being in my heart a replacement poor clay\r\n",
      "might not be lonely till later, for the fair woman and many\r\n",
      "sweetness of the Incorporated Law Society. Jonathan and I sank back in the\r\n",
      "day. We now the parkon me, I know, forgive me if I seem egotistical.\"\r\n",
      "\r\n",
      "\"Nay! perhips, they come to him\r\n",
      "just too late! But do not agree to abide by all\r\n",
      "the terms of this agreement.  There are a few\r\n",
      "things that you can do within the ences of our work is done, one the most harrowing\r\n",
      "to her death:--\r\n",
      "\r\n",
      "\"Jonathan is hard done. I can hear\r\n",
      "men's so joke, and I let my of I am not reported, for\r\n",
      "the dogs for me, and I could fancy that I could have loved him for it. Here it\r\n",
      "is....\r\n",
      "\r\n",
      "     Strange that it never struck me that the very next of the sofa. He was very\r\n",
      "courteous and vevotion in that time when you saw her you shall come and see lunatic--I might adventure in such a red mark. I have tried to be useful to Jonathan,\r\n",
      "And so have done so, for I was prepared to sign of him. I looked\r\n",
      "dead in the morning lay and follow him, so I said:--\r\n",
      "\r\n",
      "\"But will not the worst it child. There, in one thing:\r\n",
      "she was conscious, and whithersoed in time. There must be no chance long the rats and his own\r\n",
      "entrance of the house, copied from the face of the more of a terrible\r\n",
      "experience in Transylvania. I thought that if there were no fearful ordeal. There are many belongings\r\n",
      "that he must be a good fellow of the ship. I got out that it was here, on the coach, peered eagerly into the\r\n",
      "amersing to get out. The coffin was empty.\r\n",
      "\r\n",
      "I remember now that he did not tell me anything of his castle, both he had sing\r\n",
      "to blow. He went back to the room, and found Van Helsing recalls it to his pittle back. The Professor says that if we can so treat of falling water, and the\r\n",
      "door to the left, and had tallyed it again?\" He held\r\n",
      "out the letter to me, and with a considerable pause, and then long sun. He learn the horses and made all ready. Madam Mina slept sinke\r\n",
      "of the seat in the daylight to dark yew-trees at the side of the churchyard whilst he would watch at\r\n",
      "the operation. As the transfusion of his left with me, for if he evidently it could\r\n",
      "be no standing on his began to whom I was because I would save Madam Mina from that awful\r\n",
      "place that I would go. God forbid taken up with his\r\n",
      "reason, and a puzzled look spread over his face as, she seiz so far from us to his arms. We then to hear it faculty which enables us. The Professor and I looked at each other in a foreign\r\n",
      "schooner with a sigh he so dreads I cannot understand it, but she is not. She is all right, and I\r\n",
      "heard the crow of Eeep\r\n",
      "     they are right, and I cannot above the horizon. If I want anything I\r\n",
      "shall call out to see him at the incident of the boxes, and from a wolf couldn't be an instant the poor dear\r\n",
      "child mind somewhere. I have returned of a\r\n",
      "boy. He saw me at once, and there is nothing like since Jonathan had struck him. I thought it wiser down on her more than before.\r\n",
      "She did so that he\r\n",
      "wanted us to do, but especially addressing himself to Arthur, so that no marks the dear\r\n",
      "grew back and found you shaking my body is certainly something to poor Miss Westenra\r\n",
      "to-morrow again. Somewheres in an agony of her lips and gums and on the\r\n",
      "door thing put out two forth in paragraph 1.E.1\r\n",
      "\r\n",
      "1.E.3.  LIMALETOG JoUREAP--\r\n",
      "\r\n",
      "_2 October._--A whole as I was so frightened that I crept it be too late. God's will be\r\n",
      "done!\" Down came another like a horrible laugh, and raising himself to hide and covered\r\n",
      "him at the last the same of my side. The fact step she kissed it. \"Ah\r\n",
      "the door of the door, he leaned over and solemnly broken\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "\n",
    "print(generate_text(model, 5000, .6, \"Dracula moved to Copenhagen because \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "id": "9q6NKUhHMERD",
    "outputId": "da23836a-f285-482d-9802-b9a431592b6c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAJQCAYAAAAg+ngHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "loss = history.history['loss']\n",
    "newLoss = np.squeeze(loss)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(newLoss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Generation with RNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
