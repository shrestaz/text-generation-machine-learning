{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import numpy as np # vectorization\n",
    "import random # generating text\n",
    "import tensorflow as tf # ML\n",
    "import datetime # clock training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Text length in number of characters:  5662324\n",
      "\n",
      " First 1000 characters of the book: \n",
      " \n",
      "\n",
      "“We should start back,” Gared urged as the woods began to grow dark around them. “The wildlings are dead.”\n",
      "\n",
      "“Do the dead frighten you?” Ser Waymar Royce asked with just the hint of a smile.\n",
      "\n",
      "Gared did not rise to the bait. He was an old man, past fifty, and he had seen the lordlings come and go. “Dead is dead,” he said. “We have no business with the dead.”\n",
      "\n",
      "“Are they dead?” Royce asked softly. “What proof have we?”\n",
      "\n",
      "“Will saw them,” Gared said. “If he says they are dead, that’s proof enough for me.”\n",
      "\n",
      "Will had known they would drag him into the quarrel sooner or later. He wished it had been later rather than sooner. “My mother told me that dead men sing no songs,” he put in.\n",
      "\n",
      "“My wet nurse said the same thing, Will,” Royce replied. “Never believe anything you hear at a woman’s tit. There are things to be learned even from the dead.” His voice echoed, too loud in the twilit forest.\n",
      "\n",
      "“We have a long ride before us,” Gared pointed out. “Eight days, maybe nine. And night is falling.”\n",
      "\n",
      "Ser\n"
     ]
    }
   ],
   "source": [
    "# Import the book\n",
    "\n",
    "book = tf.keras.utils.get_file('iceAndFire', 'https://raw.githubusercontent.com/nihitx/game-of-thrones-/master/gameofthrones.txt')\n",
    "text = open(book, 'rb').read().decode(encoding='utf-8') # open(file, mode = rb ('r' means open for reading, 'b' means in binary mode. When opening file in bonaru mode, non-ASCII characters, newlines are not transformed. Tldr: it doesn't harm the data))\n",
    "print('\\n Text length in number of characters: ', len(text))\n",
    "print('\\n First 1000 characters of the book: \\n', text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  86\n",
      "['\\n', ' ', '!', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', 'é', 'ê', '—', '‘', '’', '“', '”', '…']\n"
     ]
    }
   ],
   "source": [
    "# Sort the unique characters used and print them\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_size = len(chars)\n",
    "print('Number of characters: ', char_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '(': 3, ')': 4, ',': 5, '-': 6, '.': 7, '/': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, ';': 20, '?': 21, 'A': 22, 'B': 23, 'C': 24, 'D': 25, 'E': 26, 'F': 27, 'G': 28, 'H': 29, 'I': 30, 'J': 31, 'K': 32, 'L': 33, 'M': 34, 'N': 35, 'O': 36, 'P': 37, 'Q': 38, 'R': 39, 'S': 40, 'T': 41, 'U': 42, 'V': 43, 'W': 44, 'X': 45, 'Y': 46, 'Z': 47, '[': 48, ']': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '{': 76, '}': 77, 'é': 78, 'ê': 79, '—': 80, '‘': 81, '’': 82, '“': 83, '”': 84, '…': 85}\n",
      "------------\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '(', 4: ')', 5: ',', 6: '-', 7: '.', 8: '/', 9: '0', 10: '1', 11: '2', 12: '3', 13: '4', 14: '5', 15: '6', 16: '7', 17: '8', 18: '9', 19: ':', 20: ';', 21: '?', 22: 'A', 23: 'B', 24: 'C', 25: 'D', 26: 'E', 27: 'F', 28: 'G', 29: 'H', 30: 'I', 31: 'J', 32: 'K', 33: 'L', 34: 'M', 35: 'N', 36: 'O', 37: 'P', 38: 'Q', 39: 'R', 40: 'S', 41: 'T', 42: 'U', 43: 'V', 44: 'W', 45: 'X', 46: 'Y', 47: 'Z', 48: '[', 49: ']', 50: 'a', 51: 'b', 52: 'c', 53: 'd', 54: 'e', 55: 'f', 56: 'g', 57: 'h', 58: 'i', 59: 'j', 60: 'k', 61: 'l', 62: 'm', 63: 'n', 64: 'o', 65: 'p', 66: 'q', 67: 'r', 68: 's', 69: 't', 70: 'u', 71: 'v', 72: 'w', 73: 'x', 74: 'y', 75: 'z', 76: '{', 77: '}', 78: 'é', 79: 'ê', 80: '—', 81: '‘', 82: '’', 83: '“', 84: '”', 85: '…'}\n"
     ]
    }
   ],
   "source": [
    "# Convert charactes to id and vice-versa in a dictionary. Basically, indexing the characters as key:value pair\n",
    "\n",
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "#id2char = dict(for c, i in enumerate(chars))\n",
    "id2char = dict(enumerate(chars))\n",
    "print(char2id1)\n",
    "print('------------')\n",
    "\n",
    "print(id2char)\n",
    "#print('------------')\n",
    "#print(id2char)\n",
    "#for c, i in enumerate(chars):\n",
    "#    print (i, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the probability of each next character\n",
    "def sample(prediction): # prediction is the list of possible characters. This method will return the most likely character\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0 # store prediction character\n",
    "    char_id = len(prediction) - 1\n",
    "    # For each character prediction probability\n",
    "    for i in range(len(prediction)):\n",
    "        s += prediction[i]\n",
    "        if s >= r:\n",
    "            char_id = i\n",
    "            break\n",
    "    \n",
    "    # One hot encoding the probable characters. One hot encoding is to differentiate, not rank values. i.e 00001000\n",
    "    char_one_hot = np.zeros(prediction.shape) # Return a new array of given shape and type, filled with zeros.\n",
    "    char_one_hot[char_id] = 1.0 # set the value of probable character to 1\n",
    "    return char_one_hot # returns something like 0000010000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize our data to feed it into the model\n",
    "\n",
    "len_per_section = 50 # length perception is the size of batch which we'll feed into the model. In this case, 50 character long batches.\n",
    "skip = 10 # Fill the section list with chunks of characters. After the first batch, skip the first 10 character and create a new batch.\n",
    "         # First batch: Hello World.\n",
    "         # Second batch: llo World. I ....and so on.\n",
    "         # Can be 'skipped' if there's a lot of text\n",
    "sections = []\n",
    "next_chars = []\n",
    "\n",
    "\n",
    "for i in range(0, len(text) -len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])\n",
    "    \n",
    "# Vectorize input and output\n",
    "\n",
    "# Matrix of section lenght by num of characters\n",
    "x = np.zeros((len(sections), len_per_section, char_size))\n",
    "# label column for all the character id's still zero\n",
    "y = np.zeros((len(sections), char_size))\n",
    "\n",
    "# for each char in each section, convert each char to an ID\n",
    "# for each section, convert the labels to ids\n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        x[i, j, char2id[char]] = 1\n",
    "    y[i, char2id[next_chars[i]]] = 1\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:  566228\n",
      "approximate steps per epoch:  1105\n"
     ]
    }
   ],
   "source": [
    "# Machine learning time\n",
    "\n",
    "batch_size = 512 # batch size is a number of training samples. Higher the batch size, higher the memory usage.\n",
    "max_steps = 1\n",
    "log_every = 100\n",
    "save_every = 6000\n",
    "hidden_nodes = 1024\n",
    "\n",
    "test_start = 'You know nothing Jon Snow.'\n",
    "\n",
    "# save our model\n",
    "checkpoint_directory = 'ckpt'\n",
    "\n",
    "# Create a checkpoint directory\n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size: ', len(x))\n",
    "print('approximate steps per epoch: ', int(len(x)/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thatonedroid/anaconda3/envs/iceAndFire/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-3b90cfec1f45>:104: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build our model\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    global_step = tf.Variable(0) # global step refers to the number of batches that are seen by the graph.\n",
    "    \n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size]) # tensor size is 3 dimensonal. This is the data we are going to feed into our model.\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "# Input gate, output gate, forget gate, internal state\n",
    "# They will be calculated in vacuums\n",
    "    \n",
    "    # Input gate - weights for input, previous output and bias\n",
    "    w_ii = tf.Variable(tf.truncated_normal ([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal ([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # Forget gate\n",
    "    w_fi = tf.Variable(tf.truncated_normal ([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal ([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # Output gate\n",
    "    w_oi = tf.Variable(tf.truncated_normal ([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal ([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # Memory cell (hidden state)\n",
    "    w_ci = tf.Variable(tf.truncated_normal ([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal ([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    def lstm(i, o, state):\n",
    "        \n",
    "        # These are all calculated seperately, no overlap until....\n",
    "        # bias prevents over-fitting\n",
    "        # (input * input weights) + (output * weights for previous output) + bias\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        \n",
    "        # (input * forget weights) + (output * weights for previous output) + bias\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        \n",
    "        # (input * output weights) + (output * weights for previous output) + bias\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        \n",
    "        # (input * internal state weights) + (output * weights for previous output) + bias\n",
    "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        \n",
    "        # ...now! Multiply forget gate * given state    +  input gate * hidden state\n",
    "        state = forget_gate * state + input_gate * memory_cell\n",
    "        # squash that state with tanh nonlin (Computes hyperbolic tangent of x element-wise)\n",
    "        \n",
    "        # multiply by output\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        # return \n",
    "        return output, state\n",
    "    \n",
    "    ###########\n",
    "    #Operation\n",
    "    ###########\n",
    "    #LSTM\n",
    "    #both start off as empty, LSTM will calculate this\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "\n",
    "    #unrolled LSTM loop\n",
    "    #for each input set\n",
    "    for i in range(len_per_section):\n",
    "        #calculate state and output from LSTM\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        #to start, \n",
    "        if i == 0:\n",
    "            #store initial output and labels\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        #for each new set, concat outputs and labels\n",
    "        elif i != len_per_section - 1:\n",
    "            #concatenates (combines) vectors along a dimension axis, not multiply\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, data[:, i+1, :]], 0)\n",
    "        else:\n",
    "            #final store\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, labels], 0)\n",
    "        \n",
    "    #Classifier\n",
    "    #The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    \n",
    "    #calculate weight and bias values for the network\n",
    "    #generated randomly given a size and distribution\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "    #Logits simply means that the function operates on the unscaled output \n",
    "    #of earlier layers and that the relative scale to understand the units \n",
    "    #is linear. It means, in particular, the sum of the inputs may not equal 1, \n",
    "    #that the values are not probabilities (you might have an input of 5).\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    \n",
    "    #logits is our prediction outputs, lets compare it with our labels\n",
    "    #cross entropy since multiclass classification\n",
    "    #computes the cost for a softmax layer\n",
    "    #then Computes the mean of elements across dimensions of a tensor.\n",
    "    #average loss across all values\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels_all_i, logits = logits))\n",
    "\n",
    "    #Optimizer\n",
    "    #minimize loss with graident descent, learning rate 10,  keep track of batches\n",
    "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    ###########\n",
    "    #Test\n",
    "    ###########\n",
    "    test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
    "    test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #Reset at the beginning of each test\n",
    "    reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
    "                                test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "    #LSTM\n",
    "    test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 4.45 (2019-04-17 09:34:26.929867)\n"
     ]
    }
   ],
   "source": [
    "#timew to train the model, initialize a session with a graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #standard init step\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #for each training step\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #starts off as 0\n",
    "        offset = offset % len(x)\n",
    "        \n",
    "        #calculate batch data and labels to feed model iteratively\n",
    "        if offset <= (len(x) - batch_size):\n",
    "            #first part\n",
    "            batch_data = x[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        #until when offset  = batch size, then we \n",
    "        else:\n",
    "            #last part\n",
    "            to_add = batch_size - (len(x) - offset)\n",
    "            batch_data = np.concatenate((x[offset: len(x)], x[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset: len(x)], y[0: to_add]))\n",
    "            offset = to_add\n",
    "        \n",
    "        #optimize!!\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "\n",
    "            if step % save_every == 0:\n",
    "                saver.save(sess, checkpoint_directory + '/model', global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/model-0\n",
      "Y\n",
      "o\n",
      "u\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "J\n",
      "o\n",
      "n\n",
      " \n",
      "S\n",
      "n\n",
      "o\n",
      "w\n",
      "You know nothing Jon Snow        e   e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_start = 'You know nothing Jon Snow '\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #init graph, load model\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)\n",
    "\n",
    "    #set input variable to generate chars from\n",
    "    reset_test_state.run() \n",
    "    test_generated = test_start\n",
    "\n",
    "    #for every char in the input sentennce\n",
    "    for i in range(len(test_start) - 1):\n",
    "        #initialize an empty char store\n",
    "        test_X = np.zeros((1, char_size))\n",
    "        #store it in id from\n",
    "        print(test_start[i])\n",
    "        test_X[0, char2id[test_start[i]]] = 1.\n",
    "        #feed it to model, test_prediction is the output value\n",
    "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "\n",
    "    \n",
    "    #where we store encoded char predictions\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    test_X[0, char2id[test_start[-1]]] = 1.\n",
    "\n",
    "    #lets generate 500 characters\n",
    "    for i in range(500):\n",
    "        #get each prediction probability\n",
    "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "        #one hot encode it\n",
    "        next_char_one_hot = sample(prediction)\n",
    "        #get the indices of the max values (highest probability)  and convert to char\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        #add each char to the output text iteratively\n",
    "        test_generated += next_char\n",
    "        #update the \n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    print(test_generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
