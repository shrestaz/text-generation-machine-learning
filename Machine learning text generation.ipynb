{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import numpy as np # vectorization\n",
    "import random # generating text\n",
    "import tensorflow as tf # ML\n",
    "import datetime # clock training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Text length in number of characters:  5662324\n",
      "\n",
      " First 1000 characters of the book: \n",
      " \n",
      "\n",
      "“We should start back,” Gared urged as the woods began to grow dark around them. “The wildlings are dead.”\n",
      "\n",
      "“Do the dead frighten you?” Ser Waymar Royce asked with just the hint of a smile.\n",
      "\n",
      "Gared did not rise to the bait. He was an old man, past fifty, and he had seen the lordlings come and go. “Dead is dead,” he said. “We have no business with the dead.”\n",
      "\n",
      "“Are they dead?” Royce asked softly. “What proof have we?”\n",
      "\n",
      "“Will saw them,” Gared said. “If he says they are dead, that’s proof enough for me.”\n",
      "\n",
      "Will had known they would drag him into the quarrel sooner or later. He wished it had been later rather than sooner. “My mother told me that dead men sing no songs,” he put in.\n",
      "\n",
      "“My wet nurse said the same thing, Will,” Royce replied. “Never believe anything you hear at a woman’s tit. There are things to be learned even from the dead.” His voice echoed, too loud in the twilit forest.\n",
      "\n",
      "“We have a long ride before us,” Gared pointed out. “Eight days, maybe nine. And night is falling.”\n",
      "\n",
      "Ser\n"
     ]
    }
   ],
   "source": [
    "# Import the book\n",
    "\n",
    "book = tf.keras.utils.get_file('iceAndFire', 'https://raw.githubusercontent.com/nihitx/game-of-thrones-/master/gameofthrones.txt')\n",
    "text = open(book, 'rb').read().decode(encoding='utf-8') # open(file, mode = rb ('r' means open for reading, 'b' means in binary mode. When opening file in bonaru mode, non-ASCII characters, newlines are not transformed. Tldr: it doesn't harm the data))\n",
    "print('\\n Text length in number of characters: ', len(text))\n",
    "print('\\n First 1000 characters of the book: \\n', text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  86\n",
      "['\\n', ' ', '!', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', 'é', 'ê', '—', '‘', '’', '“', '”', '…']\n"
     ]
    }
   ],
   "source": [
    "# Sort the unique characters used and print them\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_size = len(chars)\n",
    "print('Number of characters: ', char_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert charactes to id and vice-versa in a dictionary. Basically, indexing the characters as key:value pair\n",
    "\n",
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2char = dict((i, c) for c, i in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the probability of each next character\n",
    "def sample(prediction): # prediction is the list of possible characters. This method will return the most likely character\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0 # store prediction character\n",
    "    char_id = len(prediction) - 1\n",
    "    # For each character prediction probability\n",
    "    for i in range(len(prediction)):\n",
    "        s += prediction[i]\n",
    "        if s >= r:\n",
    "            char_id = i\n",
    "            break\n",
    "    \n",
    "    # One hot encoding the probable characters. One hot encoding is to differentiate, not rank values. i.e 00001000\n",
    "    char_one_hot = np.zeros(shape[char_size]) # Return a new array of given shape and type, filled with zeros.\n",
    "    char_one_hot[char_id] = 1.0 # set the value of probable character to 1\n",
    "    return char_one_hot # returns something like 0000010000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize our data to feed it into the model\n",
    "\n",
    "len_per_section = 50 # length perception is the size of batch which we'll feed into the model. In this case, 50 character long batches.\n",
    "skip = 10 # Fill the section list with chunks of characters. After the first batch, skip the first 10 character and create a new batch.\n",
    "         # First batch: Hello World.\n",
    "         # Second batch: llo World. I ....and so on.\n",
    "         # Can be 'skipped' if there's a lot of text\n",
    "sections = []\n",
    "next_chars = []\n",
    "\n",
    "\n",
    "for i in range(0, len(text) -len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])\n",
    "    \n",
    "# Vectorize input and output\n",
    "\n",
    "# Matrix of section lenght by num of characters\n",
    "x = np.zeros((len(sections), len_per_section, char_size))\n",
    "# label column for all the character id's still zero\n",
    "y = np.zeros((len(sections), char_size))\n",
    "\n",
    "# for each char in each section, convert each char to an ID\n",
    "# for each section, convert the labels to ids\n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        x[i, j, char2id[char]] = 1\n",
    "    y[i, char2id[next_chars[i]]] = 1\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:  566228\n",
      "approximate steps per epoch:  1105\n"
     ]
    }
   ],
   "source": [
    "# Machine learning time\n",
    "\n",
    "batch_size = 512 # batch size is a number of training samples. Higher the batch size, higher the memory usage.\n",
    "max_steps = 70000\n",
    "log_every = 100\n",
    "save_every = 6000\n",
    "hidden_nodes = 1024\n",
    "\n",
    "test_start = 'You know nothing Jon Snow.'\n",
    "\n",
    "# save our model\n",
    "checkpoint_directory = 'ckpt'\n",
    "\n",
    "# Create a checkpoint directory\n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size: ', len(x))\n",
    "print('approximate steps per epoch: ', int(len(x)/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thatonedroid/anaconda3/envs/iceAndFire/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-26779e4da7e2>:104: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build our model\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    global_step = tf.Variable(0) # global step refers to the number of batches that are seen by the graph.\n",
    "    \n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size]) # tensor size is 3 dimensonal. This is the data we are going to feed into our model.\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "# Input gate, output gate, forget gate, internal state\n",
    "# They will be calculated in vacuums\n",
    "    \n",
    "    # Input gate - weights for input, previous output and bias\n",
    "    w_ii = tf.Variable(tf.truncated_normal ([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal ([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # Forget gate\n",
    "    w_fi = tf.Variable(tf.truncated_normal ([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal ([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # Output gate\n",
    "    w_oi = tf.Variable(tf.truncated_normal ([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal ([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    # Memory cell (hidden state)\n",
    "    w_ci = tf.Variable(tf.truncated_normal ([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal ([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    def lstm(i, o, state):\n",
    "        \n",
    "        # These are all calculated seperately, no overlap until....\n",
    "        # bias prevents over-fitting\n",
    "        # (input * input weights) + (output * weights for previous output) + bias\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        \n",
    "        # (input * forget weights) + (output * weights for previous output) + bias\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        \n",
    "        # (input * output weights) + (output * weights for previous output) + bias\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        \n",
    "        # (input * internal state weights) + (output * weights for previous output) + bias\n",
    "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        \n",
    "        # ...now! Multiply forget gate * given state    +  input gate * hidden state\n",
    "        state = forget_gate * state + input_gate * memory_cell\n",
    "        # squash that state with tanh nonlin (Computes hyperbolic tangent of x element-wise)\n",
    "        \n",
    "        # multiply by output\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        # return \n",
    "        return output, state\n",
    "    \n",
    "    ###########\n",
    "    #Operation\n",
    "    ###########\n",
    "    #LSTM\n",
    "    #both start off as empty, LSTM will calculate this\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "\n",
    "    #unrolled LSTM loop\n",
    "    #for each input set\n",
    "    for i in range(len_per_section):\n",
    "        #calculate state and output from LSTM\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        #to start, \n",
    "        if i == 0:\n",
    "            #store initial output and labels\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        #for each new set, concat outputs and labels\n",
    "        elif i != len_per_section - 1:\n",
    "            #concatenates (combines) vectors along a dimension axis, not multiply\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, data[:, i+1, :]], 0)\n",
    "        else:\n",
    "            #final store\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, labels], 0)\n",
    "        \n",
    "    #Classifier\n",
    "    #The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    \n",
    "    #calculate weight and bias values for the network\n",
    "    #generated randomly given a size and distribution\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "    #Logits simply means that the function operates on the unscaled output \n",
    "    #of earlier layers and that the relative scale to understand the units \n",
    "    #is linear. It means, in particular, the sum of the inputs may not equal 1, \n",
    "    #that the values are not probabilities (you might have an input of 5).\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    \n",
    "    #logits is our prediction outputs, lets compare it with our labels\n",
    "    #cross entropy since multiclass classification\n",
    "    #computes the cost for a softmax layer\n",
    "    #then Computes the mean of elements across dimensions of a tensor.\n",
    "    #average loss across all values\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels_all_i, logits = logits))\n",
    "\n",
    "    #Optimizer\n",
    "    #minimize loss with graident descent, learning rate 10,  keep track of batches\n",
    "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    ###########\n",
    "    #Test\n",
    "    ###########\n",
    "    #test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
    "    #test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #Reset at the beginning of each test\n",
    "    #reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
    "                                #test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "    #LSTM\n",
    "    #test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "    #test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 4.49 (2019-04-16 22:21:22.071485)\n",
      "training loss at step 10: 4.29 (2019-04-16 22:22:07.153337)\n",
      "training loss at step 20: 4.54 (2019-04-16 22:22:54.300467)\n",
      "training loss at step 30: 3.25 (2019-04-16 22:23:39.741818)\n",
      "training loss at step 40: 4.19 (2019-04-16 22:24:25.853301)\n",
      "training loss at step 50: 3.32 (2019-04-16 22:25:10.793697)\n",
      "training loss at step 60: 3.23 (2019-04-16 22:25:56.310326)\n",
      "training loss at step 70: 3.12 (2019-04-16 22:26:43.853531)\n",
      "training loss at step 80: 3.05 (2019-04-16 22:27:36.554555)\n",
      "training loss at step 90: 3.00 (2019-04-16 22:28:27.076167)\n",
      "training loss at step 100: 3.04 (2019-04-16 22:29:13.600818)\n",
      "training loss at step 110: 3.48 (2019-04-16 22:30:02.959250)\n",
      "training loss at step 120: 3.14 (2019-04-16 22:30:51.094072)\n",
      "training loss at step 130: 2.92 (2019-04-16 22:31:39.439409)\n",
      "training loss at step 140: 2.98 (2019-04-16 22:32:26.334369)\n",
      "training loss at step 150: 2.98 (2019-04-16 22:33:13.602621)\n",
      "training loss at step 160: 3.01 (2019-04-16 22:34:01.810317)\n",
      "training loss at step 170: 3.04 (2019-04-16 22:34:51.197430)\n",
      "training loss at step 180: 3.09 (2019-04-16 22:35:52.482520)\n",
      "training loss at step 190: 3.26 (2019-04-16 22:36:51.804673)\n",
      "training loss at step 200: 3.18 (2019-04-16 22:37:42.729403)\n",
      "training loss at step 210: 2.99 (2019-04-16 22:38:35.560561)\n",
      "training loss at step 220: 2.92 (2019-04-16 22:39:23.047889)\n",
      "training loss at step 230: 2.99 (2019-04-16 22:40:11.893163)\n",
      "training loss at step 240: 2.93 (2019-04-16 22:40:58.834123)\n",
      "training loss at step 250: 3.22 (2019-04-16 22:41:44.043261)\n",
      "training loss at step 260: 3.14 (2019-04-16 22:42:31.864530)\n",
      "training loss at step 270: 2.93 (2019-04-16 22:43:20.765222)\n",
      "training loss at step 280: 3.12 (2019-04-16 22:44:31.300332)\n",
      "training loss at step 290: 3.17 (2019-04-16 22:45:43.674598)\n",
      "training loss at step 300: 3.16 (2019-04-16 22:46:39.729847)\n",
      "training loss at step 310: 3.89 (2019-04-16 22:47:32.576593)\n",
      "training loss at step 320: 3.01 (2019-04-16 22:48:24.255958)\n",
      "training loss at step 330: 3.18 (2019-04-16 22:49:20.463342)\n",
      "training loss at step 340: 3.12 (2019-04-16 22:50:10.706627)\n",
      "training loss at step 350: 3.03 (2019-04-16 22:51:08.467768)\n",
      "training loss at step 360: 3.14 (2019-04-16 22:52:01.372895)\n",
      "training loss at step 370: 3.17 (2019-04-16 22:52:50.508422)\n",
      "training loss at step 380: 2.91 (2019-04-16 22:53:37.901395)\n",
      "training loss at step 390: 3.04 (2019-04-16 22:54:22.700937)\n",
      "training loss at step 400: 3.04 (2019-04-16 22:55:06.093071)\n",
      "training loss at step 410: 3.05 (2019-04-16 22:55:49.856595)\n",
      "training loss at step 420: 3.00 (2019-04-16 22:56:33.411055)\n",
      "training loss at step 430: 2.94 (2019-04-16 22:57:17.158035)\n",
      "training loss at step 440: 2.96 (2019-04-16 22:58:02.939852)\n",
      "training loss at step 450: 2.95 (2019-04-16 22:58:50.222291)\n",
      "training loss at step 460: 2.92 (2019-04-16 22:59:36.554496)\n",
      "training loss at step 470: 3.02 (2019-04-16 23:00:23.334491)\n",
      "training loss at step 480: 3.04 (2019-04-16 23:01:08.025981)\n",
      "training loss at step 490: 2.94 (2019-04-16 23:01:51.755731)\n",
      "training loss at step 500: 2.99 (2019-04-16 23:02:35.388603)\n",
      "training loss at step 510: 2.95 (2019-04-16 23:03:19.558253)\n",
      "training loss at step 520: 2.96 (2019-04-16 23:04:04.418480)\n",
      "training loss at step 530: 2.95 (2019-04-16 23:04:48.733719)\n",
      "training loss at step 540: 2.90 (2019-04-16 23:05:33.319578)\n",
      "training loss at step 550: 2.88 (2019-04-16 23:06:19.012975)\n",
      "training loss at step 560: 2.86 (2019-04-16 23:07:05.108674)\n",
      "training loss at step 570: 2.96 (2019-04-16 23:07:59.616522)\n",
      "training loss at step 580: 2.90 (2019-04-16 23:08:49.689517)\n",
      "training loss at step 590: 2.96 (2019-04-16 23:09:52.398580)\n",
      "training loss at step 600: 2.88 (2019-04-16 23:10:59.509596)\n",
      "training loss at step 610: 2.98 (2019-04-16 23:11:52.107693)\n",
      "training loss at step 620: 2.96 (2019-04-16 23:12:59.422907)\n",
      "training loss at step 630: 2.91 (2019-04-16 23:14:05.092364)\n",
      "training loss at step 640: 2.91 (2019-04-16 23:15:01.819374)\n",
      "training loss at step 650: 2.87 (2019-04-16 23:15:54.809152)\n",
      "training loss at step 660: 2.85 (2019-04-16 23:16:49.647307)\n",
      "training loss at step 670: 2.88 (2019-04-16 23:17:54.365188)\n",
      "training loss at step 680: 2.92 (2019-04-16 23:18:50.382006)\n",
      "training loss at step 690: 2.89 (2019-04-16 23:19:49.569835)\n",
      "training loss at step 700: 2.81 (2019-04-16 23:20:44.452077)\n",
      "training loss at step 710: 2.82 (2019-04-16 23:21:29.784270)\n",
      "training loss at step 720: 2.85 (2019-04-16 23:22:12.604122)\n",
      "training loss at step 730: 2.89 (2019-04-16 23:22:55.417186)\n",
      "training loss at step 740: 2.83 (2019-04-16 23:23:38.358550)\n",
      "training loss at step 750: 2.81 (2019-04-16 23:24:21.201792)\n",
      "training loss at step 760: 2.88 (2019-04-16 23:25:04.154141)\n",
      "training loss at step 770: 2.85 (2019-04-16 23:25:46.660342)\n",
      "training loss at step 780: 2.82 (2019-04-16 23:26:29.404099)\n",
      "training loss at step 790: 2.83 (2019-04-16 23:27:12.237493)\n",
      "training loss at step 800: 2.85 (2019-04-16 23:27:55.041223)\n",
      "training loss at step 810: 2.87 (2019-04-16 23:28:38.433280)\n",
      "training loss at step 820: 2.81 (2019-04-16 23:29:21.198300)\n",
      "training loss at step 830: 2.85 (2019-04-16 23:30:03.808546)\n",
      "training loss at step 840: 2.91 (2019-04-16 23:30:46.611387)\n",
      "training loss at step 850: 2.93 (2019-04-16 23:31:29.502290)\n",
      "training loss at step 860: 2.93 (2019-04-16 23:32:12.389658)\n",
      "training loss at step 870: 2.82 (2019-04-16 23:32:55.289193)\n",
      "training loss at step 880: 2.80 (2019-04-16 23:33:38.263439)\n",
      "training loss at step 890: 2.93 (2019-04-16 23:34:21.024187)\n",
      "training loss at step 900: 2.88 (2019-04-16 23:35:03.960630)\n",
      "training loss at step 910: 2.81 (2019-04-16 23:35:46.929668)\n",
      "training loss at step 920: 2.89 (2019-04-16 23:36:29.632916)\n",
      "training loss at step 930: 2.89 (2019-04-16 23:37:12.479682)\n",
      "training loss at step 940: 2.85 (2019-04-16 23:37:55.334077)\n",
      "training loss at step 950: 2.79 (2019-04-16 23:38:38.705369)\n",
      "training loss at step 960: 2.81 (2019-04-16 23:39:21.177313)\n",
      "training loss at step 970: 2.90 (2019-04-16 23:40:03.802216)\n",
      "training loss at step 980: 2.88 (2019-04-16 23:40:46.677238)\n",
      "training loss at step 990: 2.86 (2019-04-16 23:41:29.722501)\n",
      "training loss at step 1000: 2.80 (2019-04-16 23:42:11.904930)\n",
      "training loss at step 1010: 2.88 (2019-04-16 23:42:53.605933)\n",
      "training loss at step 1020: 2.90 (2019-04-16 23:43:35.139724)\n",
      "training loss at step 1030: 2.88 (2019-04-16 23:44:16.283567)\n",
      "training loss at step 1040: 2.87 (2019-04-16 23:44:58.495222)\n",
      "training loss at step 1050: 2.88 (2019-04-16 23:45:41.408520)\n",
      "training loss at step 1060: 2.81 (2019-04-16 23:46:24.553999)\n",
      "training loss at step 1070: 2.83 (2019-04-16 23:47:07.618313)\n",
      "training loss at step 1080: 2.85 (2019-04-16 23:47:50.568943)\n",
      "training loss at step 1090: 2.92 (2019-04-16 23:48:33.797502)\n",
      "training loss at step 1100: 3.42 (2019-04-16 23:49:17.088891)\n",
      "training loss at step 1110: 2.81 (2019-04-16 23:50:00.122433)\n",
      "training loss at step 1120: 2.81 (2019-04-16 23:50:43.287029)\n",
      "training loss at step 1130: 2.83 (2019-04-16 23:51:26.180034)\n",
      "training loss at step 1140: 2.83 (2019-04-16 23:52:09.069194)\n",
      "training loss at step 1150: 2.82 (2019-04-16 23:52:52.156991)\n",
      "training loss at step 1160: 2.82 (2019-04-16 23:53:35.537640)\n",
      "training loss at step 1170: 2.90 (2019-04-16 23:54:18.464976)\n",
      "training loss at step 1180: 2.86 (2019-04-16 23:55:01.445808)\n",
      "training loss at step 1190: 2.84 (2019-04-16 23:55:44.882362)\n",
      "training loss at step 1200: 2.80 (2019-04-16 23:56:28.810683)\n",
      "training loss at step 1210: 2.84 (2019-04-16 23:57:13.389357)\n",
      "training loss at step 1220: 2.84 (2019-04-16 23:57:56.930761)\n",
      "training loss at step 1230: 2.80 (2019-04-16 23:58:40.999476)\n",
      "training loss at step 1240: 2.94 (2019-04-16 23:59:24.180168)\n",
      "training loss at step 1250: 2.86 (2019-04-17 00:00:07.248112)\n",
      "training loss at step 1260: 2.86 (2019-04-17 00:00:57.319614)\n",
      "training loss at step 1270: 2.82 (2019-04-17 00:01:50.293031)\n",
      "training loss at step 1280: 2.85 (2019-04-17 00:02:44.563393)\n",
      "training loss at step 1290: 2.83 (2019-04-17 00:03:38.242848)\n",
      "training loss at step 1300: 2.85 (2019-04-17 00:04:31.874421)\n",
      "training loss at step 1310: 2.77 (2019-04-17 00:05:25.220028)\n",
      "training loss at step 1320: 2.85 (2019-04-17 00:06:18.594780)\n",
      "training loss at step 1330: 2.84 (2019-04-17 00:07:12.120556)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 1340: 2.87 (2019-04-17 00:08:03.371093)\n",
      "training loss at step 1350: 2.81 (2019-04-17 00:08:46.316928)\n",
      "training loss at step 1360: 2.82 (2019-04-17 00:09:28.866232)\n",
      "training loss at step 1370: 2.80 (2019-04-17 00:10:11.386633)\n",
      "training loss at step 1380: 2.80 (2019-04-17 00:10:54.084870)\n",
      "training loss at step 1390: 2.76 (2019-04-17 00:11:36.594397)\n",
      "training loss at step 1400: 2.88 (2019-04-17 00:12:19.166130)\n",
      "training loss at step 1410: 2.84 (2019-04-17 00:13:01.787075)\n",
      "training loss at step 1420: 2.79 (2019-04-17 00:13:44.204699)\n",
      "training loss at step 1430: 2.82 (2019-04-17 00:14:25.699464)\n",
      "training loss at step 1440: 2.84 (2019-04-17 00:15:07.267175)\n",
      "training loss at step 1450: 2.85 (2019-04-17 00:15:48.695329)\n",
      "training loss at step 1460: 2.82 (2019-04-17 00:16:30.181141)\n",
      "training loss at step 1470: 2.80 (2019-04-17 00:17:11.514287)\n",
      "training loss at step 1480: 2.83 (2019-04-17 00:17:52.907983)\n",
      "training loss at step 1490: 2.81 (2019-04-17 00:18:34.570734)\n",
      "training loss at step 1500: 2.85 (2019-04-17 00:19:17.078397)\n",
      "training loss at step 1510: 2.81 (2019-04-17 00:20:00.214168)\n",
      "training loss at step 1520: 2.87 (2019-04-17 00:20:43.018062)\n",
      "training loss at step 1530: 2.84 (2019-04-17 00:21:25.951478)\n",
      "training loss at step 1540: 2.81 (2019-04-17 00:22:09.320270)\n",
      "training loss at step 1550: 2.83 (2019-04-17 00:22:52.181167)\n",
      "training loss at step 1560: 2.85 (2019-04-17 00:23:35.375068)\n",
      "training loss at step 1570: 2.80 (2019-04-17 00:24:18.388356)\n",
      "training loss at step 1580: 2.83 (2019-04-17 00:25:01.606107)\n",
      "training loss at step 1590: 2.81 (2019-04-17 00:25:44.684206)\n",
      "training loss at step 1600: 2.79 (2019-04-17 00:26:27.738314)\n",
      "training loss at step 1610: 2.79 (2019-04-17 00:27:10.711102)\n",
      "training loss at step 1620: 2.80 (2019-04-17 00:27:53.887090)\n",
      "training loss at step 1630: 2.84 (2019-04-17 00:28:37.243999)\n",
      "training loss at step 1640: 2.86 (2019-04-17 00:29:20.114436)\n",
      "training loss at step 1650: 2.75 (2019-04-17 00:30:03.115993)\n",
      "training loss at step 1660: 2.83 (2019-04-17 00:30:46.066877)\n",
      "training loss at step 1670: 2.82 (2019-04-17 00:31:29.339122)\n",
      "training loss at step 1680: 2.80 (2019-04-17 00:32:12.477496)\n",
      "training loss at step 1690: 2.87 (2019-04-17 00:32:55.644552)\n",
      "training loss at step 1700: 2.81 (2019-04-17 00:33:38.985892)\n",
      "training loss at step 1710: 2.82 (2019-04-17 00:34:22.087142)\n",
      "training loss at step 1720: 2.82 (2019-04-17 00:35:05.143874)\n",
      "training loss at step 1730: 2.81 (2019-04-17 00:35:48.050802)\n",
      "training loss at step 1740: 2.82 (2019-04-17 00:36:30.932946)\n",
      "training loss at step 1750: 2.78 (2019-04-17 00:37:13.721617)\n",
      "training loss at step 1760: 3.46 (2019-04-17 00:37:56.664061)\n",
      "training loss at step 1770: 2.80 (2019-04-17 00:38:40.045920)\n",
      "training loss at step 1780: 2.78 (2019-04-17 00:39:22.663714)\n",
      "training loss at step 1790: 2.80 (2019-04-17 00:40:05.612370)\n",
      "training loss at step 1800: 2.82 (2019-04-17 00:40:48.693626)\n",
      "training loss at step 1810: 2.85 (2019-04-17 00:41:31.489847)\n",
      "training loss at step 1820: 2.81 (2019-04-17 00:42:14.301602)\n",
      "training loss at step 1830: 2.78 (2019-04-17 00:42:57.134201)\n",
      "training loss at step 1840: 2.79 (2019-04-17 00:43:40.364420)\n",
      "training loss at step 1850: 2.69 (2019-04-17 00:44:23.130482)\n",
      "training loss at step 1860: 2.80 (2019-04-17 00:45:06.261878)\n",
      "training loss at step 1870: 2.74 (2019-04-17 00:45:49.175422)\n",
      "training loss at step 1880: 2.71 (2019-04-17 00:46:32.119807)\n",
      "training loss at step 1890: 2.68 (2019-04-17 00:47:15.674828)\n",
      "training loss at step 1900: 2.68 (2019-04-17 00:47:59.396553)\n",
      "training loss at step 1910: 2.68 (2019-04-17 00:48:42.816596)\n",
      "training loss at step 1920: 2.66 (2019-04-17 00:49:26.066722)\n",
      "training loss at step 1930: 2.61 (2019-04-17 00:50:10.029483)\n",
      "training loss at step 1940: 2.72 (2019-04-17 00:50:53.694527)\n",
      "training loss at step 1950: 2.62 (2019-04-17 00:51:36.915314)\n",
      "training loss at step 1960: 2.62 (2019-04-17 00:52:20.219761)\n",
      "training loss at step 1970: 2.60 (2019-04-17 00:53:03.788093)\n",
      "training loss at step 1980: 2.66 (2019-04-17 00:53:48.711839)\n",
      "training loss at step 1990: 2.62 (2019-04-17 00:54:32.287401)\n",
      "training loss at step 2000: 2.68 (2019-04-17 00:55:15.289553)\n",
      "training loss at step 2010: 2.58 (2019-04-17 00:55:58.252806)\n",
      "training loss at step 2020: 2.57 (2019-04-17 00:56:41.398292)\n",
      "training loss at step 2030: 2.71 (2019-04-17 00:57:24.703464)\n",
      "training loss at step 2040: 2.58 (2019-04-17 00:58:08.976974)\n",
      "training loss at step 2050: 2.62 (2019-04-17 00:58:53.012958)\n",
      "training loss at step 2060: 2.52 (2019-04-17 00:59:37.448833)\n",
      "training loss at step 2070: 2.58 (2019-04-17 01:00:22.636840)\n",
      "training loss at step 2080: 2.55 (2019-04-17 01:01:05.917779)\n",
      "training loss at step 2090: 2.49 (2019-04-17 01:01:48.875792)\n",
      "training loss at step 2100: 2.49 (2019-04-17 01:02:31.912399)\n",
      "training loss at step 2110: 2.45 (2019-04-17 01:03:14.729313)\n",
      "training loss at step 2120: 2.62 (2019-04-17 01:03:57.539863)\n",
      "training loss at step 2130: 2.52 (2019-04-17 01:04:40.307652)\n",
      "training loss at step 2140: 2.61 (2019-04-17 01:05:22.939319)\n",
      "training loss at step 2150: 2.49 (2019-04-17 01:06:06.349013)\n",
      "training loss at step 2160: 2.47 (2019-04-17 01:06:49.301732)\n",
      "training loss at step 2170: 2.49 (2019-04-17 01:07:32.535718)\n",
      "training loss at step 2180: 2.47 (2019-04-17 01:08:15.620691)\n",
      "training loss at step 2190: 2.43 (2019-04-17 01:08:58.951988)\n",
      "training loss at step 2200: 2.47 (2019-04-17 01:09:42.141537)\n",
      "training loss at step 2210: 3.28 (2019-04-17 01:10:25.235690)\n",
      "training loss at step 2220: 2.40 (2019-04-17 01:11:07.936742)\n",
      "training loss at step 2230: 2.42 (2019-04-17 01:11:51.042743)\n",
      "training loss at step 2240: 2.38 (2019-04-17 01:12:33.938116)\n",
      "training loss at step 2250: 2.43 (2019-04-17 01:13:17.123970)\n",
      "training loss at step 2260: 2.40 (2019-04-17 01:14:00.644518)\n",
      "training loss at step 2270: 2.43 (2019-04-17 01:14:43.555758)\n",
      "training loss at step 2280: 2.43 (2019-04-17 01:15:26.508081)\n",
      "training loss at step 2290: 2.39 (2019-04-17 01:16:09.747438)\n",
      "training loss at step 2300: 2.40 (2019-04-17 01:16:52.782632)\n",
      "training loss at step 2310: 2.46 (2019-04-17 01:17:35.692117)\n",
      "training loss at step 2320: 2.41 (2019-04-17 01:18:18.623207)\n",
      "training loss at step 2330: 2.43 (2019-04-17 01:19:01.098521)\n",
      "training loss at step 2340: 2.37 (2019-04-17 01:19:43.818131)\n",
      "training loss at step 2350: 2.40 (2019-04-17 01:20:26.396578)\n",
      "training loss at step 2360: 2.46 (2019-04-17 01:21:08.757372)\n",
      "training loss at step 2370: 2.38 (2019-04-17 01:21:51.488570)\n",
      "training loss at step 2380: 2.40 (2019-04-17 01:22:34.227996)\n",
      "training loss at step 2390: 2.44 (2019-04-17 01:23:16.583521)\n",
      "training loss at step 2400: 2.39 (2019-04-17 01:23:59.815377)\n",
      "training loss at step 2410: 2.43 (2019-04-17 01:24:42.242297)\n",
      "training loss at step 2420: 2.42 (2019-04-17 01:25:24.444598)\n",
      "training loss at step 2430: 2.35 (2019-04-17 01:26:06.769131)\n",
      "training loss at step 2440: 2.37 (2019-04-17 01:26:49.255204)\n",
      "training loss at step 2450: 2.44 (2019-04-17 01:27:31.598545)\n",
      "training loss at step 2460: 2.41 (2019-04-17 01:28:14.279229)\n",
      "training loss at step 2470: 2.41 (2019-04-17 01:28:57.416889)\n",
      "training loss at step 2480: 2.40 (2019-04-17 01:29:40.331811)\n",
      "training loss at step 2490: 2.34 (2019-04-17 01:30:23.082210)\n",
      "training loss at step 2500: 2.33 (2019-04-17 01:31:05.813509)\n",
      "training loss at step 2510: 2.37 (2019-04-17 01:31:48.779046)\n",
      "training loss at step 2520: 2.35 (2019-04-17 01:32:32.009909)\n",
      "training loss at step 2530: 2.37 (2019-04-17 01:33:15.002094)\n",
      "training loss at step 2540: 2.40 (2019-04-17 01:33:58.100897)\n",
      "training loss at step 2550: 2.32 (2019-04-17 01:34:40.776428)\n",
      "training loss at step 2560: 2.38 (2019-04-17 01:35:23.510917)\n",
      "training loss at step 2570: 2.39 (2019-04-17 01:36:06.223378)\n",
      "training loss at step 2580: 2.37 (2019-04-17 01:36:49.144975)\n",
      "training loss at step 2590: 2.32 (2019-04-17 01:37:32.039079)\n",
      "training loss at step 2600: 2.37 (2019-04-17 01:38:14.968718)\n",
      "training loss at step 2610: 2.34 (2019-04-17 01:38:58.314956)\n",
      "training loss at step 2620: 2.43 (2019-04-17 01:39:41.388034)\n",
      "training loss at step 2630: 2.38 (2019-04-17 01:40:24.149134)\n",
      "training loss at step 2640: 2.33 (2019-04-17 01:41:07.177048)\n",
      "training loss at step 2650: 2.35 (2019-04-17 01:41:50.338754)\n",
      "training loss at step 2660: 2.36 (2019-04-17 01:42:33.372800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 2670: 2.39 (2019-04-17 01:43:16.148788)\n",
      "training loss at step 2680: 2.33 (2019-04-17 01:43:59.269950)\n",
      "training loss at step 2690: 2.37 (2019-04-17 01:44:41.977858)\n",
      "training loss at step 2700: 2.35 (2019-04-17 01:45:24.528325)\n",
      "training loss at step 2710: 2.41 (2019-04-17 01:46:07.209511)\n",
      "training loss at step 2720: 2.33 (2019-04-17 01:46:49.482929)\n",
      "training loss at step 2730: 2.39 (2019-04-17 01:47:31.889339)\n",
      "training loss at step 2740: 2.35 (2019-04-17 01:48:14.213856)\n",
      "training loss at step 2750: 2.32 (2019-04-17 01:48:57.165846)\n",
      "training loss at step 2760: 2.32 (2019-04-17 01:49:39.767297)\n",
      "training loss at step 2770: 2.27 (2019-04-17 01:50:22.228122)\n",
      "training loss at step 2780: 2.29 (2019-04-17 01:51:04.967657)\n",
      "training loss at step 2790: 2.23 (2019-04-17 01:51:48.351557)\n",
      "training loss at step 2800: 2.30 (2019-04-17 01:52:31.698354)\n",
      "training loss at step 2810: 2.35 (2019-04-17 01:53:15.329004)\n",
      "training loss at step 2820: 2.30 (2019-04-17 01:53:58.193092)\n",
      "training loss at step 2830: 2.33 (2019-04-17 01:54:41.242597)\n",
      "training loss at step 2840: 2.24 (2019-04-17 01:55:24.282725)\n",
      "training loss at step 2850: 2.21 (2019-04-17 01:56:07.162802)\n",
      "training loss at step 2860: 2.26 (2019-04-17 01:56:50.390668)\n",
      "training loss at step 2870: 2.77 (2019-04-17 01:57:33.661523)\n",
      "training loss at step 2880: 2.30 (2019-04-17 01:58:16.776757)\n",
      "training loss at step 2890: 2.26 (2019-04-17 01:59:00.124260)\n",
      "training loss at step 2900: 2.27 (2019-04-17 01:59:43.255613)\n",
      "training loss at step 2910: 2.24 (2019-04-17 02:00:26.388635)\n",
      "training loss at step 2920: 2.29 (2019-04-17 02:01:09.442212)\n",
      "training loss at step 2930: 2.23 (2019-04-17 02:01:52.433747)\n",
      "training loss at step 2940: 2.26 (2019-04-17 02:02:35.410809)\n",
      "training loss at step 2950: 2.28 (2019-04-17 02:03:18.619827)\n",
      "training loss at step 2960: 2.28 (2019-04-17 02:04:01.769122)\n",
      "training loss at step 2970: 2.43 (2019-04-17 02:04:44.560445)\n",
      "training loss at step 2980: 2.28 (2019-04-17 02:05:27.641993)\n",
      "training loss at step 2990: 2.29 (2019-04-17 02:06:10.721499)\n",
      "training loss at step 3000: 2.24 (2019-04-17 02:06:53.535986)\n",
      "training loss at step 3010: 2.23 (2019-04-17 02:07:36.651464)\n",
      "training loss at step 3020: 2.26 (2019-04-17 02:08:19.724968)\n",
      "training loss at step 3030: 2.23 (2019-04-17 02:09:02.704520)\n",
      "training loss at step 3040: 2.27 (2019-04-17 02:09:45.943235)\n",
      "training loss at step 3050: 2.22 (2019-04-17 02:10:28.820336)\n",
      "training loss at step 3060: 2.29 (2019-04-17 02:11:11.892030)\n",
      "training loss at step 3070: 2.30 (2019-04-17 02:11:54.811932)\n",
      "training loss at step 3080: 2.27 (2019-04-17 02:12:37.666586)\n",
      "training loss at step 3090: 2.20 (2019-04-17 02:13:20.539567)\n",
      "training loss at step 3100: 2.21 (2019-04-17 02:14:03.173412)\n",
      "training loss at step 3110: 2.20 (2019-04-17 02:14:45.612244)\n",
      "training loss at step 3120: 2.21 (2019-04-17 02:15:28.423423)\n",
      "training loss at step 3130: 2.27 (2019-04-17 02:16:11.263960)\n",
      "training loss at step 3140: 2.21 (2019-04-17 02:16:54.107009)\n",
      "training loss at step 3150: 2.27 (2019-04-17 02:17:36.847204)\n",
      "training loss at step 3160: 2.21 (2019-04-17 02:18:19.493644)\n",
      "training loss at step 3170: 2.18 (2019-04-17 02:19:02.250062)\n",
      "training loss at step 3180: 2.21 (2019-04-17 02:19:44.774414)\n",
      "training loss at step 3190: 2.25 (2019-04-17 02:20:27.241205)\n",
      "training loss at step 3200: 2.23 (2019-04-17 02:21:09.929357)\n",
      "training loss at step 3210: 2.18 (2019-04-17 02:21:52.724322)\n",
      "training loss at step 3220: 2.23 (2019-04-17 02:22:35.515686)\n",
      "training loss at step 3230: 2.17 (2019-04-17 02:23:18.420318)\n",
      "training loss at step 3240: 2.16 (2019-04-17 02:24:01.591509)\n",
      "training loss at step 3250: 2.24 (2019-04-17 02:24:44.178214)\n",
      "training loss at step 3260: 2.16 (2019-04-17 02:25:26.952518)\n",
      "training loss at step 3270: 2.19 (2019-04-17 02:26:09.620681)\n",
      "training loss at step 3280: 2.16 (2019-04-17 02:26:52.353027)\n",
      "training loss at step 3290: 2.16 (2019-04-17 02:27:35.115418)\n",
      "training loss at step 3300: 2.20 (2019-04-17 02:28:17.847898)\n",
      "training loss at step 3310: 3.10 (2019-04-17 02:29:00.947208)\n",
      "training loss at step 3320: 2.18 (2019-04-17 02:29:43.863766)\n",
      "training loss at step 3330: 2.18 (2019-04-17 02:30:26.583543)\n",
      "training loss at step 3340: 2.20 (2019-04-17 02:31:09.176718)\n",
      "training loss at step 3350: 2.16 (2019-04-17 02:31:51.890648)\n",
      "training loss at step 3360: 2.15 (2019-04-17 02:32:34.408901)\n",
      "training loss at step 3370: 2.21 (2019-04-17 02:33:16.812179)\n",
      "training loss at step 3380: 2.21 (2019-04-17 02:34:00.034230)\n",
      "training loss at step 3390: 2.16 (2019-04-17 02:34:43.089913)\n",
      "training loss at step 3400: 2.15 (2019-04-17 02:35:25.581863)\n",
      "training loss at step 3410: 2.11 (2019-04-17 02:36:08.646902)\n",
      "training loss at step 3420: 2.17 (2019-04-17 02:36:51.689693)\n",
      "training loss at step 3430: 2.12 (2019-04-17 02:37:34.934132)\n",
      "training loss at step 3440: 2.18 (2019-04-17 02:38:17.960312)\n",
      "training loss at step 3450: 2.12 (2019-04-17 02:39:01.144189)\n",
      "training loss at step 3460: 2.07 (2019-04-17 02:39:43.890420)\n",
      "training loss at step 3470: 2.13 (2019-04-17 02:40:26.523771)\n",
      "training loss at step 3480: 2.16 (2019-04-17 02:41:09.063957)\n",
      "training loss at step 3490: 2.17 (2019-04-17 02:41:51.645589)\n",
      "training loss at step 3500: 2.16 (2019-04-17 02:42:34.350295)\n",
      "training loss at step 3510: 2.11 (2019-04-17 02:43:17.277527)\n",
      "training loss at step 3520: 2.10 (2019-04-17 02:44:00.244617)\n",
      "training loss at step 3530: 2.15 (2019-04-17 02:44:43.424663)\n",
      "training loss at step 3540: 2.06 (2019-04-17 02:45:26.769474)\n",
      "training loss at step 3550: 2.13 (2019-04-17 02:46:10.185936)\n",
      "training loss at step 3560: 2.08 (2019-04-17 02:46:53.501736)\n",
      "training loss at step 3570: 2.12 (2019-04-17 02:47:37.283380)\n",
      "training loss at step 3580: 2.09 (2019-04-17 02:48:20.182457)\n",
      "training loss at step 3590: 2.13 (2019-04-17 02:49:03.590988)\n",
      "training loss at step 3600: 2.12 (2019-04-17 02:49:46.563949)\n",
      "training loss at step 3610: 2.07 (2019-04-17 02:50:29.456236)\n",
      "training loss at step 3620: 2.06 (2019-04-17 02:51:12.311341)\n",
      "training loss at step 3630: 2.60 (2019-04-17 02:51:55.341524)\n",
      "training loss at step 3640: 2.09 (2019-04-17 02:52:38.502166)\n",
      "training loss at step 3650: 2.12 (2019-04-17 02:53:21.292749)\n",
      "training loss at step 3660: 2.09 (2019-04-17 02:54:04.862391)\n",
      "training loss at step 3670: 2.14 (2019-04-17 02:54:47.789040)\n",
      "training loss at step 3680: 2.10 (2019-04-17 02:55:30.672562)\n",
      "training loss at step 3690: 2.09 (2019-04-17 02:56:13.756684)\n",
      "training loss at step 3700: 2.11 (2019-04-17 02:56:56.807921)\n",
      "training loss at step 3710: 2.09 (2019-04-17 02:57:39.503530)\n",
      "training loss at step 3720: 2.11 (2019-04-17 02:58:22.165678)\n",
      "training loss at step 3730: 2.05 (2019-04-17 02:59:04.673629)\n",
      "training loss at step 3740: 2.15 (2019-04-17 02:59:47.344897)\n",
      "training loss at step 3750: 2.10 (2019-04-17 03:00:29.912934)\n",
      "training loss at step 3760: 2.07 (2019-04-17 03:01:12.862868)\n",
      "training loss at step 3770: 2.11 (2019-04-17 03:01:55.759823)\n",
      "training loss at step 3780: 2.16 (2019-04-17 03:02:38.381896)\n",
      "training loss at step 3790: 2.07 (2019-04-17 03:03:21.467271)\n",
      "training loss at step 3800: 2.06 (2019-04-17 03:04:04.713421)\n",
      "training loss at step 3810: 2.11 (2019-04-17 03:04:47.477149)\n",
      "training loss at step 3820: 2.07 (2019-04-17 03:05:29.993026)\n",
      "training loss at step 3830: 2.11 (2019-04-17 03:06:12.480140)\n",
      "training loss at step 3840: 2.05 (2019-04-17 03:06:55.342760)\n",
      "training loss at step 3850: 2.05 (2019-04-17 03:07:38.082015)\n",
      "training loss at step 3860: 2.03 (2019-04-17 03:08:20.774824)\n",
      "training loss at step 3870: 2.08 (2019-04-17 03:09:04.406089)\n",
      "training loss at step 3880: 2.11 (2019-04-17 03:09:47.154834)\n",
      "training loss at step 3890: 2.00 (2019-04-17 03:10:29.531702)\n",
      "training loss at step 3900: 2.03 (2019-04-17 03:11:12.442723)\n",
      "training loss at step 3910: 2.03 (2019-04-17 03:11:55.177131)\n",
      "training loss at step 3920: 2.06 (2019-04-17 03:12:37.765998)\n",
      "training loss at step 3930: 2.02 (2019-04-17 03:13:20.048386)\n",
      "training loss at step 3940: 2.08 (2019-04-17 03:14:03.120960)\n",
      "training loss at step 3950: 2.08 (2019-04-17 03:14:45.371070)\n",
      "training loss at step 3960: 2.07 (2019-04-17 03:15:27.735150)\n",
      "training loss at step 3970: 3.30 (2019-04-17 03:16:09.961906)\n",
      "training loss at step 3980: 2.03 (2019-04-17 03:16:52.577642)\n",
      "training loss at step 3990: 2.07 (2019-04-17 03:17:35.103057)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 4000: 2.04 (2019-04-17 03:18:17.868460)\n",
      "training loss at step 4010: 2.52 (2019-04-17 03:19:00.594529)\n",
      "training loss at step 4020: 2.08 (2019-04-17 03:19:43.221810)\n",
      "training loss at step 4030: 2.05 (2019-04-17 03:20:26.211202)\n",
      "training loss at step 4040: 2.04 (2019-04-17 03:21:09.133040)\n",
      "training loss at step 4050: 2.06 (2019-04-17 03:21:52.077575)\n",
      "training loss at step 4060: 2.01 (2019-04-17 03:22:34.810900)\n",
      "training loss at step 4070: 1.95 (2019-04-17 03:23:17.252294)\n",
      "training loss at step 4080: 2.08 (2019-04-17 03:24:00.304726)\n",
      "training loss at step 4090: 2.07 (2019-04-17 03:24:42.688812)\n",
      "training loss at step 4100: 2.02 (2019-04-17 03:25:25.052516)\n",
      "training loss at step 4110: 1.92 (2019-04-17 03:26:07.310961)\n",
      "training loss at step 4120: 2.01 (2019-04-17 03:26:49.472574)\n",
      "training loss at step 4130: 1.98 (2019-04-17 03:27:31.974698)\n",
      "training loss at step 4140: 2.07 (2019-04-17 03:28:14.442249)\n",
      "training loss at step 4150: 2.02 (2019-04-17 03:28:57.029377)\n",
      "training loss at step 4160: 2.01 (2019-04-17 03:29:39.692168)\n",
      "training loss at step 4170: 2.07 (2019-04-17 03:30:22.178207)\n",
      "training loss at step 4180: 2.05 (2019-04-17 03:31:04.823094)\n",
      "training loss at step 4190: 1.97 (2019-04-17 03:31:47.376344)\n",
      "training loss at step 4200: 2.08 (2019-04-17 03:32:29.936092)\n",
      "training loss at step 4210: 1.96 (2019-04-17 03:33:12.682778)\n",
      "training loss at step 4220: 2.06 (2019-04-17 03:33:56.319073)\n",
      "training loss at step 4230: 2.00 (2019-04-17 03:34:39.018890)\n",
      "training loss at step 4240: 2.02 (2019-04-17 03:35:21.784278)\n",
      "training loss at step 4250: 2.08 (2019-04-17 03:36:04.845157)\n",
      "training loss at step 4260: 2.02 (2019-04-17 03:36:47.667055)\n",
      "training loss at step 4270: 1.93 (2019-04-17 03:37:30.146300)\n",
      "training loss at step 4280: 2.07 (2019-04-17 03:38:12.732598)\n",
      "training loss at step 4290: 2.03 (2019-04-17 03:38:55.377283)\n",
      "training loss at step 4300: 1.96 (2019-04-17 03:39:37.872415)\n",
      "training loss at step 4310: 2.03 (2019-04-17 03:40:20.330348)\n",
      "training loss at step 4320: 1.91 (2019-04-17 03:41:02.840487)\n",
      "training loss at step 4330: 1.98 (2019-04-17 03:41:45.390148)\n",
      "training loss at step 4340: 1.98 (2019-04-17 03:42:28.134854)\n",
      "training loss at step 4350: 1.98 (2019-04-17 03:43:11.033784)\n",
      "training loss at step 4360: 1.99 (2019-04-17 03:43:54.106592)\n",
      "training loss at step 4370: 2.01 (2019-04-17 03:44:37.058574)\n",
      "training loss at step 4380: 1.97 (2019-04-17 03:45:19.750071)\n",
      "training loss at step 4390: 1.96 (2019-04-17 03:46:02.834988)\n",
      "training loss at step 4400: 1.99 (2019-04-17 03:46:45.790886)\n",
      "training loss at step 4410: 1.98 (2019-04-17 03:47:28.894755)\n",
      "training loss at step 4420: 2.60 (2019-04-17 03:48:12.051438)\n",
      "training loss at step 4430: 1.95 (2019-04-17 03:48:56.195589)\n",
      "training loss at step 4440: 2.01 (2019-04-17 03:49:38.776955)\n",
      "training loss at step 4450: 2.02 (2019-04-17 03:50:21.382509)\n",
      "training loss at step 4460: 2.00 (2019-04-17 03:51:03.902548)\n",
      "training loss at step 4470: 1.91 (2019-04-17 03:51:46.424709)\n",
      "training loss at step 4480: 1.96 (2019-04-17 03:52:28.906822)\n",
      "training loss at step 4490: 1.96 (2019-04-17 03:53:11.914555)\n",
      "training loss at step 4500: 1.97 (2019-04-17 03:53:54.598058)\n",
      "training loss at step 4510: 1.97 (2019-04-17 03:54:37.423348)\n",
      "training loss at step 4520: 1.95 (2019-04-17 03:55:20.337888)\n",
      "training loss at step 4530: 1.96 (2019-04-17 03:56:03.436996)\n",
      "training loss at step 4540: 1.94 (2019-04-17 03:56:46.524216)\n",
      "training loss at step 4550: 2.02 (2019-04-17 03:57:29.249303)\n",
      "training loss at step 4560: 2.00 (2019-04-17 03:58:11.993120)\n",
      "training loss at step 4570: 1.94 (2019-04-17 03:58:55.792227)\n",
      "training loss at step 4580: 1.95 (2019-04-17 03:59:38.474484)\n",
      "training loss at step 4590: 2.04 (2019-04-17 04:00:20.968019)\n",
      "training loss at step 4600: 2.03 (2019-04-17 04:01:03.830800)\n",
      "training loss at step 4610: 1.94 (2019-04-17 04:01:46.419634)\n",
      "training loss at step 4620: 1.96 (2019-04-17 04:02:29.309963)\n",
      "training loss at step 4630: 1.91 (2019-04-17 04:03:12.333971)\n",
      "training loss at step 4640: 1.93 (2019-04-17 04:03:55.273651)\n",
      "training loss at step 4650: 1.97 (2019-04-17 04:04:38.255249)\n",
      "training loss at step 4660: 1.95 (2019-04-17 04:05:21.293230)\n",
      "training loss at step 4670: 1.97 (2019-04-17 04:06:04.201275)\n",
      "training loss at step 4680: 2.01 (2019-04-17 04:06:47.189902)\n",
      "training loss at step 4690: 1.88 (2019-04-17 04:07:30.091338)\n",
      "training loss at step 4700: 1.90 (2019-04-17 04:08:12.891621)\n",
      "training loss at step 4710: 1.88 (2019-04-17 04:08:56.168715)\n",
      "training loss at step 4720: 1.87 (2019-04-17 04:09:39.057797)\n",
      "training loss at step 4730: 1.98 (2019-04-17 04:10:21.959182)\n",
      "training loss at step 4740: 1.94 (2019-04-17 04:11:05.208819)\n",
      "training loss at step 4750: 1.89 (2019-04-17 04:11:48.191153)\n",
      "training loss at step 4760: 1.97 (2019-04-17 04:12:31.105699)\n",
      "training loss at step 4770: 1.98 (2019-04-17 04:13:14.108016)\n",
      "training loss at step 4780: 1.99 (2019-04-17 04:13:57.154618)\n",
      "training loss at step 4790: 1.98 (2019-04-17 04:14:39.892567)\n",
      "training loss at step 4800: 1.93 (2019-04-17 04:15:22.531461)\n",
      "training loss at step 4810: 1.97 (2019-04-17 04:16:05.238734)\n",
      "training loss at step 4820: 1.96 (2019-04-17 04:16:48.091066)\n",
      "training loss at step 4830: 1.99 (2019-04-17 04:17:30.905172)\n",
      "training loss at step 4840: 1.94 (2019-04-17 04:18:13.604336)\n",
      "training loss at step 4850: 1.96 (2019-04-17 04:18:56.630210)\n",
      "training loss at step 4860: 1.95 (2019-04-17 04:19:39.469681)\n",
      "training loss at step 4870: 1.93 (2019-04-17 04:20:22.580602)\n",
      "training loss at step 4880: 1.96 (2019-04-17 04:21:05.293385)\n",
      "training loss at step 4890: 1.92 (2019-04-17 04:21:48.105337)\n",
      "training loss at step 4900: 1.94 (2019-04-17 04:22:30.944042)\n",
      "training loss at step 4910: 1.90 (2019-04-17 04:23:13.583587)\n",
      "training loss at step 4920: 1.92 (2019-04-17 04:23:56.802463)\n",
      "training loss at step 4930: 1.94 (2019-04-17 04:24:39.355970)\n",
      "training loss at step 4940: 1.96 (2019-04-17 04:25:21.862045)\n",
      "training loss at step 4950: 1.90 (2019-04-17 04:26:04.343997)\n",
      "training loss at step 4960: 1.89 (2019-04-17 04:26:46.966945)\n",
      "training loss at step 4970: 1.91 (2019-04-17 04:27:29.437192)\n",
      "training loss at step 4980: 1.95 (2019-04-17 04:28:11.997552)\n",
      "training loss at step 4990: 1.94 (2019-04-17 04:28:55.120267)\n",
      "training loss at step 5000: 1.86 (2019-04-17 04:29:37.970708)\n",
      "training loss at step 5010: 1.92 (2019-04-17 04:30:20.783798)\n",
      "training loss at step 5020: 1.90 (2019-04-17 04:31:03.429781)\n",
      "training loss at step 5030: 1.90 (2019-04-17 04:31:46.284677)\n",
      "training loss at step 5040: 1.88 (2019-04-17 04:32:28.883580)\n",
      "training loss at step 5050: 1.89 (2019-04-17 04:33:11.250209)\n",
      "training loss at step 5060: 1.95 (2019-04-17 04:33:54.455515)\n",
      "training loss at step 5070: 1.96 (2019-04-17 04:34:37.184675)\n",
      "training loss at step 5080: 2.41 (2019-04-17 04:35:19.744968)\n",
      "training loss at step 5090: 1.96 (2019-04-17 04:36:02.987979)\n",
      "training loss at step 5100: 1.88 (2019-04-17 04:36:45.757238)\n",
      "training loss at step 5110: 1.88 (2019-04-17 04:37:28.550480)\n",
      "training loss at step 5120: 1.99 (2019-04-17 04:38:11.774000)\n",
      "training loss at step 5130: 1.83 (2019-04-17 04:38:54.883824)\n",
      "training loss at step 5140: 1.90 (2019-04-17 04:39:37.604450)\n",
      "training loss at step 5150: 1.87 (2019-04-17 04:40:19.876108)\n",
      "training loss at step 5160: 1.94 (2019-04-17 04:41:02.514327)\n",
      "training loss at step 5170: 1.86 (2019-04-17 04:41:44.873801)\n",
      "training loss at step 5180: 1.88 (2019-04-17 04:42:27.231408)\n",
      "training loss at step 5190: 1.92 (2019-04-17 04:43:09.510701)\n",
      "training loss at step 5200: 1.90 (2019-04-17 04:43:52.095715)\n",
      "training loss at step 5210: 1.91 (2019-04-17 04:44:34.376139)\n",
      "training loss at step 5220: 1.87 (2019-04-17 04:45:16.598441)\n",
      "training loss at step 5230: 2.01 (2019-04-17 04:45:59.319432)\n",
      "training loss at step 5240: 1.92 (2019-04-17 04:46:41.551118)\n",
      "training loss at step 5250: 1.93 (2019-04-17 04:47:23.917769)\n",
      "training loss at step 5260: 1.90 (2019-04-17 04:48:06.503281)\n",
      "training loss at step 5270: 1.90 (2019-04-17 04:48:49.499600)\n",
      "training loss at step 5280: 1.93 (2019-04-17 04:49:32.144486)\n",
      "training loss at step 5290: 1.89 (2019-04-17 04:50:14.680410)\n",
      "training loss at step 5300: 1.93 (2019-04-17 04:50:57.658275)\n",
      "training loss at step 5310: 1.92 (2019-04-17 04:51:40.410508)\n",
      "training loss at step 5320: 1.90 (2019-04-17 04:52:23.472078)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 5330: 1.86 (2019-04-17 04:53:06.404938)\n",
      "training loss at step 5340: 1.95 (2019-04-17 04:53:49.044387)\n",
      "training loss at step 5350: 1.86 (2019-04-17 04:54:31.695633)\n",
      "training loss at step 5360: 1.97 (2019-04-17 04:55:14.322590)\n",
      "training loss at step 5370: 1.94 (2019-04-17 04:55:56.835950)\n",
      "training loss at step 5380: 1.81 (2019-04-17 04:56:39.483012)\n",
      "training loss at step 5390: 1.89 (2019-04-17 04:57:22.052859)\n",
      "training loss at step 5400: 1.93 (2019-04-17 04:58:04.484885)\n",
      "training loss at step 5410: 1.88 (2019-04-17 04:58:47.464962)\n",
      "training loss at step 5420: 1.93 (2019-04-17 04:59:30.005730)\n",
      "training loss at step 5430: 1.89 (2019-04-17 05:00:12.646561)\n",
      "training loss at step 5440: 1.85 (2019-04-17 05:00:55.165261)\n",
      "training loss at step 5450: 1.91 (2019-04-17 05:01:37.658106)\n",
      "training loss at step 5460: 1.90 (2019-04-17 05:02:20.498536)\n",
      "training loss at step 5470: 1.90 (2019-04-17 05:03:03.343407)\n",
      "training loss at step 5480: 1.86 (2019-04-17 05:03:46.119934)\n",
      "training loss at step 5490: 1.98 (2019-04-17 05:04:28.958162)\n",
      "training loss at step 5500: 1.90 (2019-04-17 05:05:11.531767)\n",
      "training loss at step 5510: 1.86 (2019-04-17 05:05:53.957842)\n",
      "training loss at step 5520: 2.58 (2019-04-17 05:06:36.949040)\n",
      "training loss at step 5530: 2.06 (2019-04-17 05:07:19.598579)\n",
      "training loss at step 5540: 1.85 (2019-04-17 05:08:02.110374)\n",
      "training loss at step 5550: 1.86 (2019-04-17 05:08:45.279765)\n",
      "training loss at step 5560: 1.84 (2019-04-17 05:09:28.144018)\n",
      "training loss at step 5570: 1.90 (2019-04-17 05:10:10.293420)\n",
      "training loss at step 5580: 1.89 (2019-04-17 05:10:52.941665)\n",
      "training loss at step 5590: 1.87 (2019-04-17 05:11:35.304483)\n",
      "training loss at step 5600: 1.84 (2019-04-17 05:12:17.738809)\n",
      "training loss at step 5610: 1.80 (2019-04-17 05:13:00.104804)\n",
      "training loss at step 5620: 1.82 (2019-04-17 05:13:42.840971)\n",
      "training loss at step 5630: 1.91 (2019-04-17 05:14:25.279993)\n",
      "training loss at step 5640: 1.92 (2019-04-17 05:15:08.133429)\n",
      "training loss at step 5650: 1.86 (2019-04-17 05:15:50.902322)\n",
      "training loss at step 5660: 1.79 (2019-04-17 05:16:33.695875)\n",
      "training loss at step 5670: 1.89 (2019-04-17 05:17:16.863463)\n",
      "training loss at step 5680: 1.88 (2019-04-17 05:17:59.807026)\n",
      "training loss at step 5690: 1.89 (2019-04-17 05:18:43.102387)\n",
      "training loss at step 5700: 1.85 (2019-04-17 05:19:26.188978)\n",
      "training loss at step 5710: 1.85 (2019-04-17 05:20:09.276719)\n",
      "training loss at step 5720: 1.84 (2019-04-17 05:20:51.946800)\n",
      "training loss at step 5730: 1.86 (2019-04-17 05:21:34.512054)\n",
      "training loss at step 5740: 1.85 (2019-04-17 05:22:17.227200)\n",
      "training loss at step 5750: 1.88 (2019-04-17 05:22:59.875121)\n",
      "training loss at step 5760: 1.83 (2019-04-17 05:23:42.576489)\n",
      "training loss at step 5770: 1.78 (2019-04-17 05:24:25.177856)\n",
      "training loss at step 5780: 1.82 (2019-04-17 05:25:08.082813)\n",
      "training loss at step 5790: 1.80 (2019-04-17 05:25:51.311601)\n",
      "training loss at step 5800: 1.94 (2019-04-17 05:26:34.185707)\n",
      "training loss at step 5810: 1.83 (2019-04-17 05:27:17.299266)\n",
      "training loss at step 5820: 1.89 (2019-04-17 05:27:59.902649)\n",
      "training loss at step 5830: 1.84 (2019-04-17 05:28:43.044281)\n",
      "training loss at step 5840: 2.53 (2019-04-17 05:29:25.946527)\n",
      "training loss at step 5850: 1.83 (2019-04-17 05:30:08.948022)\n",
      "training loss at step 5860: 1.89 (2019-04-17 05:30:51.807383)\n",
      "training loss at step 5870: 1.89 (2019-04-17 05:31:34.759021)\n",
      "training loss at step 5880: 1.84 (2019-04-17 05:32:17.636767)\n",
      "training loss at step 5890: 1.88 (2019-04-17 05:33:00.228220)\n",
      "training loss at step 5900: 1.82 (2019-04-17 05:33:43.250163)\n",
      "training loss at step 5910: 1.81 (2019-04-17 05:34:25.727365)\n",
      "training loss at step 5920: 1.83 (2019-04-17 05:35:08.167050)\n",
      "training loss at step 5930: 1.94 (2019-04-17 05:35:50.612050)\n",
      "training loss at step 5940: 1.89 (2019-04-17 05:36:32.803431)\n",
      "training loss at step 5950: 1.81 (2019-04-17 05:37:15.136702)\n",
      "training loss at step 5960: 1.79 (2019-04-17 05:37:57.685857)\n",
      "training loss at step 5970: 1.91 (2019-04-17 05:38:40.318683)\n",
      "training loss at step 5980: 1.82 (2019-04-17 05:39:22.983266)\n",
      "training loss at step 5990: 1.85 (2019-04-17 05:40:05.337927)\n",
      "training loss at step 6000: 1.83 (2019-04-17 05:40:47.918837)\n",
      "training loss at step 6010: 1.83 (2019-04-17 05:41:34.915343)\n",
      "training loss at step 6020: 1.84 (2019-04-17 05:42:18.383774)\n",
      "training loss at step 6030: 1.88 (2019-04-17 05:43:01.526283)\n",
      "training loss at step 6040: 1.92 (2019-04-17 05:43:45.110832)\n",
      "training loss at step 6050: 1.85 (2019-04-17 05:44:28.212549)\n",
      "training loss at step 6060: 1.81 (2019-04-17 05:45:10.919479)\n",
      "training loss at step 6070: 1.85 (2019-04-17 05:45:53.833987)\n",
      "training loss at step 6080: 1.85 (2019-04-17 05:46:36.498987)\n",
      "training loss at step 6090: 1.82 (2019-04-17 05:47:18.857095)\n",
      "training loss at step 6100: 1.86 (2019-04-17 05:48:01.129157)\n",
      "training loss at step 6110: 1.81 (2019-04-17 05:48:44.016568)\n",
      "training loss at step 6120: 1.80 (2019-04-17 05:49:26.385614)\n",
      "training loss at step 6130: 1.93 (2019-04-17 05:50:08.942061)\n",
      "training loss at step 6140: 1.80 (2019-04-17 05:50:51.482503)\n",
      "training loss at step 6150: 1.90 (2019-04-17 05:51:34.049957)\n",
      "training loss at step 6160: 1.90 (2019-04-17 05:52:16.697895)\n",
      "training loss at step 6170: 1.81 (2019-04-17 05:52:59.271951)\n",
      "training loss at step 6180: 1.81 (2019-04-17 05:53:42.127583)\n",
      "training loss at step 6190: 1.84 (2019-04-17 05:54:24.924478)\n",
      "training loss at step 6200: 1.80 (2019-04-17 05:55:07.481507)\n",
      "training loss at step 6210: 1.81 (2019-04-17 05:55:50.002275)\n",
      "training loss at step 6220: 1.95 (2019-04-17 05:56:32.906899)\n",
      "training loss at step 6230: 1.87 (2019-04-17 05:57:15.555672)\n",
      "training loss at step 6240: 1.89 (2019-04-17 05:57:58.600414)\n",
      "training loss at step 6250: 1.85 (2019-04-17 05:58:41.930100)\n",
      "training loss at step 6260: 1.84 (2019-04-17 05:59:24.721039)\n",
      "training loss at step 6270: 1.88 (2019-04-17 06:00:07.363971)\n",
      "training loss at step 6280: 1.75 (2019-04-17 06:00:49.831321)\n",
      "training loss at step 6290: 1.95 (2019-04-17 06:01:32.063414)\n",
      "training loss at step 6300: 1.79 (2019-04-17 06:02:14.589824)\n",
      "training loss at step 6310: 1.87 (2019-04-17 06:02:56.869575)\n",
      "training loss at step 6320: 1.75 (2019-04-17 06:03:39.656650)\n",
      "training loss at step 6330: 1.79 (2019-04-17 06:04:22.037686)\n",
      "training loss at step 6340: 1.84 (2019-04-17 06:05:04.341720)\n",
      "training loss at step 6350: 1.80 (2019-04-17 06:05:46.806608)\n",
      "training loss at step 6360: 1.83 (2019-04-17 06:06:28.994339)\n",
      "training loss at step 6370: 1.84 (2019-04-17 06:07:11.516395)\n",
      "training loss at step 6380: 1.91 (2019-04-17 06:07:54.632098)\n",
      "training loss at step 6390: 1.89 (2019-04-17 06:08:37.842578)\n",
      "training loss at step 6400: 1.83 (2019-04-17 06:09:20.584598)\n",
      "training loss at step 6410: 1.85 (2019-04-17 06:10:03.662266)\n",
      "training loss at step 6420: 1.83 (2019-04-17 06:10:46.769794)\n",
      "training loss at step 6430: 1.88 (2019-04-17 06:11:29.151919)\n",
      "training loss at step 6440: 1.78 (2019-04-17 06:12:11.782899)\n",
      "training loss at step 6450: 1.89 (2019-04-17 06:12:54.655242)\n",
      "training loss at step 6460: 1.88 (2019-04-17 06:13:38.767038)\n",
      "training loss at step 6470: 1.83 (2019-04-17 06:14:21.181935)\n",
      "training loss at step 6480: 1.74 (2019-04-17 06:15:03.925041)\n",
      "training loss at step 6490: 1.93 (2019-04-17 06:15:47.288651)\n",
      "training loss at step 6500: 1.93 (2019-04-17 06:16:30.730460)\n",
      "training loss at step 6510: 1.79 (2019-04-17 06:17:14.071610)\n",
      "training loss at step 6520: 1.80 (2019-04-17 06:17:56.998634)\n",
      "training loss at step 6530: 1.83 (2019-04-17 06:18:40.028663)\n",
      "training loss at step 6540: 1.82 (2019-04-17 06:19:23.337746)\n",
      "training loss at step 6550: 1.86 (2019-04-17 06:20:07.552674)\n",
      "training loss at step 6560: 1.83 (2019-04-17 06:20:50.578976)\n",
      "training loss at step 6570: 1.90 (2019-04-17 06:21:33.092147)\n",
      "training loss at step 6580: 1.83 (2019-04-17 06:22:15.742095)\n",
      "training loss at step 6590: 1.74 (2019-04-17 06:22:58.334586)\n",
      "training loss at step 6600: 1.83 (2019-04-17 06:23:41.051486)\n",
      "training loss at step 6610: 1.78 (2019-04-17 06:24:23.758655)\n",
      "training loss at step 6620: 1.85 (2019-04-17 06:25:06.510994)\n",
      "training loss at step 6630: 2.34 (2019-04-17 06:25:49.111821)\n",
      "training loss at step 6640: 1.74 (2019-04-17 06:26:32.028950)\n",
      "training loss at step 6650: 1.79 (2019-04-17 06:27:15.053700)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 6660: 1.84 (2019-04-17 06:27:57.975708)\n",
      "training loss at step 6670: 1.69 (2019-04-17 06:28:40.931265)\n",
      "training loss at step 6680: 1.82 (2019-04-17 06:29:23.286873)\n",
      "training loss at step 6690: 1.79 (2019-04-17 06:30:05.927607)\n",
      "training loss at step 6700: 1.79 (2019-04-17 06:30:48.719397)\n",
      "training loss at step 6710: 1.75 (2019-04-17 06:31:31.511311)\n",
      "training loss at step 6720: 1.83 (2019-04-17 06:32:14.955702)\n",
      "training loss at step 6730: 1.81 (2019-04-17 06:32:57.865542)\n",
      "training loss at step 6740: 1.83 (2019-04-17 06:33:40.511309)\n",
      "training loss at step 6750: 1.73 (2019-04-17 06:34:23.192262)\n",
      "training loss at step 6760: 1.78 (2019-04-17 06:35:06.007751)\n",
      "training loss at step 6770: 1.82 (2019-04-17 06:35:48.597230)\n",
      "training loss at step 6780: 1.83 (2019-04-17 06:36:31.350163)\n",
      "training loss at step 6790: 1.76 (2019-04-17 06:37:14.249483)\n",
      "training loss at step 6800: 1.79 (2019-04-17 06:37:56.893928)\n",
      "training loss at step 6810: 1.77 (2019-04-17 06:38:40.323027)\n",
      "training loss at step 6820: 1.82 (2019-04-17 06:39:23.059433)\n",
      "training loss at step 6830: 1.77 (2019-04-17 06:40:06.111194)\n",
      "training loss at step 6840: 1.71 (2019-04-17 06:40:49.001958)\n",
      "training loss at step 6850: 1.71 (2019-04-17 06:41:31.805156)\n",
      "training loss at step 6860: 1.97 (2019-04-17 06:42:14.468990)\n",
      "training loss at step 6870: 1.80 (2019-04-17 06:42:57.046492)\n",
      "training loss at step 6880: 1.77 (2019-04-17 06:43:40.189108)\n",
      "training loss at step 6890: 1.84 (2019-04-17 06:44:22.697335)\n",
      "training loss at step 6900: 1.72 (2019-04-17 06:45:05.394505)\n",
      "training loss at step 6910: 1.81 (2019-04-17 06:45:48.110475)\n",
      "training loss at step 6920: 1.71 (2019-04-17 06:46:31.052986)\n",
      "training loss at step 6930: 1.81 (2019-04-17 06:47:15.135364)\n",
      "training loss at step 6940: 1.80 (2019-04-17 06:47:59.593592)\n",
      "training loss at step 6950: 1.89 (2019-04-17 06:48:46.049523)\n",
      "training loss at step 6960: 1.79 (2019-04-17 06:49:34.000382)\n",
      "training loss at step 6970: 1.79 (2019-04-17 06:50:20.032289)\n",
      "training loss at step 6980: 1.77 (2019-04-17 06:51:08.412273)\n",
      "training loss at step 6990: 1.82 (2019-04-17 06:51:55.132427)\n"
     ]
    }
   ],
   "source": [
    "#timew to train the model, initialize a session with a graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #standard init step\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #for each training step\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #starts off as 0\n",
    "        offset = offset % len(x)\n",
    "        \n",
    "        #calculate batch data and labels to feed model iteratively\n",
    "        if offset <= (len(x) - batch_size):\n",
    "            #first part\n",
    "            batch_data = x[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        #until when offset  = batch size, then we \n",
    "        else:\n",
    "            #last part\n",
    "            to_add = batch_size - (len(x) - offset)\n",
    "            batch_data = np.concatenate((x[offset: len(x)], x[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset: len(x)], y[0: to_add]))\n",
    "            offset = to_add\n",
    "        \n",
    "        #optimize!!\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "\n",
    "            if step % save_every == 0:\n",
    "                saver.save(sess, checkpoint_directory + '/model', global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_start = 'You know nothing Jon Snow '\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #init graph, load model\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)\n",
    "\n",
    "    #set input variable to generate chars from\n",
    "    reset_test_state.run() \n",
    "    test_generated = test_start\n",
    "\n",
    "    #for every char in the input sentennce\n",
    "    for i in range(len(test_start) - 1):\n",
    "        #initialize an empty char store\n",
    "        test_X = np.zeros((1, char_size))\n",
    "        #store it in id from\n",
    "        test_X[0, char2id[test_start[i]]] = 1.\n",
    "        #feed it to model, test_prediction is the output value\n",
    "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "\n",
    "    \n",
    "    #where we store encoded char predictions\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    test_X[0, char2id[test_start[-1]]] = 1.\n",
    "\n",
    "    #lets generate 500 characters\n",
    "    for i in range(500):\n",
    "        #get each prediction probability\n",
    "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "        #one hot encode it\n",
    "        next_char_one_hot = sample(prediction)\n",
    "        #get the indices of the max values (highest probability)  and convert to char\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        #add each char to the output text iteratively\n",
    "        test_generated += next_char\n",
    "        #update the \n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    print(test_generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
