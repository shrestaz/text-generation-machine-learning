{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80xmUzUJgGWZ"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efHQ8RUcwqAi"
   },
   "outputs": [],
   "source": [
    "# Creating and converting dataset to tensorflow\n",
    "path = tf.keras.utils.get_file('dracula.txt', 'http://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
    "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "vocabulary = sorted(set(text))\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "cbCv2ccgwknz",
    "outputId": "b55f9941-703c-4cc7-9dc7-6a2fc6532fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  883114\n",
      "Total Vocabulary:  102\n"
     ]
    }
   ],
   "source": [
    "# map unique characters to indices and vice versa\n",
    "charMapping = {u:i for i, u in enumerate(vocabulary)}\n",
    "indiceMapping = np.array(vocabulary)\n",
    "text_as_int = np.array([charMapping[c] for c in text])\n",
    "\n",
    "sequence_length = 100\n",
    "examples_per_epoch = len(text)//sequence_length # floor division\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "\n",
    "num_of_characters = len(text)\n",
    "num_of_vocabulary = len(vocabulary)\n",
    "print(\"Total Characters: \", num_of_characters)\n",
    "print(\"Total Vocabulary: \", num_of_vocabulary)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gpz5u3W32hK"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE # floor division\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "sj5x42V22Y3U",
    "outputId": "52c7d370-afe2-4a92-9d6f-733df24efaa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "embedding_dimension = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "lstm = tf.keras.layers.CuDNNLSTM # fast LSTM implementation backed by cuDNN\n",
    "gru = tf.keras.layers.CuDNNGRU # fast LSTM implementation backed by cuDNN\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, units, batch_size, drop_out):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]), # Input layer\n",
    "    gru(rnn_units, # Hidden layer 1\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='lecun_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dropout(drop_out),\n",
    "    gru(rnn_units, # Hidden layer 2\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='lecun_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dropout(drop_out),\n",
    "    gru(rnn_units, # Hidden layer 3\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='lecun_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dropout(drop_out),\n",
    "    gru(rnn_units, # Hidden layer 4\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='lecun_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dropout(drop_out),\n",
    "    tf.keras.layers.Dense(vocab_size) # Output layer\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = vocabulary_size, \n",
    "  embedding_dim=embedding_dimension, \n",
    "  units=rnn_units, \n",
    "  batch_size=BATCH_SIZE,\n",
    "  drop_out=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1A0cMnYh_RK"
   },
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  \n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3645
    },
    "colab_type": "code",
    "id": "hsQZCK_-42_R",
    "outputId": "974031a5-7ad0-47b1-e883-b34768d7dda3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 100, 102)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.625014\n",
      "Epoch 1/100\n",
      "67/68 [============================>.] - ETA: 0s - loss: 3.2683WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "68/68 [==============================] - 38s 555ms/step - loss: 3.2613\n",
      "Epoch 2/100\n",
      "68/68 [==============================] - 38s 565ms/step - loss: 2.4750\n",
      "Epoch 3/100\n",
      "68/68 [==============================] - 40s 583ms/step - loss: 2.0491\n",
      "Epoch 4/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 1.7830\n",
      "Epoch 5/100\n",
      "68/68 [==============================] - 40s 584ms/step - loss: 1.6201\n",
      "Epoch 6/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 1.5201\n",
      "Epoch 7/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 1.4479\n",
      "Epoch 8/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 1.3984\n",
      "Epoch 9/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 1.3577\n",
      "Epoch 10/100\n",
      "68/68 [==============================] - 40s 585ms/step - loss: 1.3232\n",
      "Epoch 11/100\n",
      "68/68 [==============================] - 40s 581ms/step - loss: 1.2935\n",
      "Epoch 12/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 1.2659\n",
      "Epoch 13/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 1.2412\n",
      "Epoch 14/100\n",
      "68/68 [==============================] - 40s 584ms/step - loss: 1.2155\n",
      "Epoch 15/100\n",
      "68/68 [==============================] - 40s 590ms/step - loss: 1.1934\n",
      "Epoch 16/100\n",
      "68/68 [==============================] - 40s 583ms/step - loss: 1.1710\n",
      "Epoch 17/100\n",
      "68/68 [==============================] - 40s 591ms/step - loss: 1.1478\n",
      "Epoch 18/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 1.1281\n",
      "Epoch 19/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 1.1067\n",
      "Epoch 20/100\n",
      "68/68 [==============================] - 40s 585ms/step - loss: 1.0870\n",
      "Epoch 21/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 1.0670\n",
      "Epoch 22/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 1.0463\n",
      "Epoch 23/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 1.0283\n",
      "Epoch 24/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 1.0093\n",
      "Epoch 25/100\n",
      "68/68 [==============================] - 40s 585ms/step - loss: 0.9914\n",
      "Epoch 26/100\n",
      "68/68 [==============================] - 40s 590ms/step - loss: 0.9744\n",
      "Epoch 27/100\n",
      "68/68 [==============================] - 40s 591ms/step - loss: 0.9566\n",
      "Epoch 28/100\n",
      "68/68 [==============================] - 40s 584ms/step - loss: 0.9428\n",
      "Epoch 29/100\n",
      "68/68 [==============================] - 40s 583ms/step - loss: 0.9246\n",
      "Epoch 30/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 0.9112\n",
      "Epoch 31/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 0.8971\n",
      "Epoch 32/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.8824\n",
      "Epoch 33/100\n",
      "68/68 [==============================] - 40s 584ms/step - loss: 0.8691\n",
      "Epoch 34/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.8562\n",
      "Epoch 35/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.8454\n",
      "Epoch 36/100\n",
      "68/68 [==============================] - 40s 585ms/step - loss: 0.8358\n",
      "Epoch 37/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.8250\n",
      "Epoch 38/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.8133\n",
      "Epoch 39/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.8048\n",
      "Epoch 40/100\n",
      "68/68 [==============================] - 40s 584ms/step - loss: 0.7958\n",
      "Epoch 41/100\n",
      "68/68 [==============================] - 41s 601ms/step - loss: 0.7899\n",
      "Epoch 42/100\n",
      "68/68 [==============================] - 40s 583ms/step - loss: 0.7801\n",
      "Epoch 43/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.7733\n",
      "Epoch 44/100\n",
      "68/68 [==============================] - 40s 595ms/step - loss: 0.7658\n",
      "Epoch 45/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.7619\n",
      "Epoch 46/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.7558\n",
      "Epoch 47/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.7497\n",
      "Epoch 48/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 0.7417\n",
      "Epoch 49/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.7386\n",
      "Epoch 50/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.7328\n",
      "Epoch 51/100\n",
      "68/68 [==============================] - 40s 584ms/step - loss: 0.7281\n",
      "Epoch 52/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.7257\n",
      "Epoch 53/100\n",
      "68/68 [==============================] - 40s 590ms/step - loss: 0.7225\n",
      "Epoch 54/100\n",
      "68/68 [==============================] - 40s 591ms/step - loss: 0.7157\n",
      "Epoch 55/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.7119\n",
      "Epoch 56/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.7102\n",
      "Epoch 57/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.7081\n",
      "Epoch 58/100\n",
      "68/68 [==============================] - 40s 585ms/step - loss: 0.7056\n",
      "Epoch 59/100\n",
      "68/68 [==============================] - 40s 590ms/step - loss: 0.7034\n",
      "Epoch 60/100\n",
      "68/68 [==============================] - 40s 585ms/step - loss: 0.7018\n",
      "Epoch 61/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6993\n",
      "Epoch 62/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.6981\n",
      "Epoch 63/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6958\n",
      "Epoch 64/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.6938\n",
      "Epoch 65/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 0.6942\n",
      "Epoch 66/100\n",
      "68/68 [==============================] - 40s 594ms/step - loss: 0.6925\n",
      "Epoch 67/100\n",
      "68/68 [==============================] - 40s 584ms/step - loss: 0.6899\n",
      "Epoch 68/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6900\n",
      "Epoch 69/100\n",
      "68/68 [==============================] - 40s 590ms/step - loss: 0.6889\n",
      "Epoch 70/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.6897\n",
      "Epoch 71/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 0.6861\n",
      "Epoch 72/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 0.6861\n",
      "Epoch 73/100\n",
      "68/68 [==============================] - 40s 590ms/step - loss: 0.6849\n",
      "Epoch 74/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.6854\n",
      "Epoch 75/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.6854\n",
      "Epoch 76/100\n",
      "68/68 [==============================] - 40s 585ms/step - loss: 0.6857\n",
      "Epoch 77/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.6857\n",
      "Epoch 78/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.6824\n",
      "Epoch 79/100\n",
      "68/68 [==============================] - 40s 582ms/step - loss: 0.6860\n",
      "Epoch 80/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6869\n",
      "Epoch 81/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6855\n",
      "Epoch 82/100\n",
      "68/68 [==============================] - 40s 585ms/step - loss: 0.6867\n",
      "Epoch 83/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6885\n",
      "Epoch 84/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6895\n",
      "Epoch 85/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.6903\n",
      "Epoch 86/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6886\n",
      "Epoch 87/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 0.6907\n",
      "Epoch 88/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6923\n",
      "Epoch 89/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6940\n",
      "Epoch 90/100\n",
      "68/68 [==============================] - 40s 584ms/step - loss: 0.6924\n",
      "Epoch 91/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.6982\n",
      "Epoch 92/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.6977\n",
      "Epoch 93/100\n",
      "68/68 [==============================] - 40s 582ms/step - loss: 0.6997\n",
      "Epoch 94/100\n",
      "68/68 [==============================] - 40s 587ms/step - loss: 0.6996\n",
      "Epoch 95/100\n",
      "68/68 [==============================] - 40s 589ms/step - loss: 0.7002\n",
      "Epoch 96/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.7037\n",
      "Epoch 97/100\n",
      "68/68 [==============================] - 40s 592ms/step - loss: 0.7082\n",
      "Epoch 98/100\n",
      "68/68 [==============================] - 40s 586ms/step - loss: 0.7098\n",
      "Epoch 99/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.7101\n",
      "Epoch 100/100\n",
      "68/68 [==============================] - 40s 588ms/step - loss: 0.7144\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
    "\n",
    "model.compile(tf.train.AdamOptimizer(), loss=loss)\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "EPOCHS=100\n",
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrPopEcKfTbv"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocabulary_size, embedding_dimension, rnn_units, batch_size=1, drop_out=0.2)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_Evgr4Pwydc"
   },
   "outputs": [],
   "source": [
    "# Generate text algorithm\n",
    "def generate_text(model, num_char_to_generate, temperature, start_string):\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [charMapping[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_char_to_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(indiceMapping[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1498
    },
    "colab_type": "code",
    "id": "n-jikOVZ710c",
    "outputId": "87eb5a61-57cb-4957-c387-36f425863211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-b38a7cfe830f>:18: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "Dracula moved to Copenhagen because it is to me to be kept in the darkness. It was\n",
      "a startled scream, and the red light on what she was in a case,\n",
      "he was so in a very straggling in her will she made straight again. This is no more to\n",
      "be in the morning. It was a shock, for I felt that I was sure to be the\n",
      "empty house we want to go to the latter confidence. I am afraid, a vague\n",
      "terrible and a sort of chin and neck and strive the lock; and I cannot leave the land the trembled\n",
      "light on the facts. She was in such a feel like a steel view simply she did\n",
      "not report as we had seen me. But now I know what about\n",
      "them I found it over the same. When I came in the train I had now and teet, she was\n",
      "still thirsty. I knew that what are young to cheer my equally\n",
      "with any phrase--\"or what I was leaving the\n",
      "highest gentleman.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"I tried to say that she will not be getting so grateful to you! I shall tell you of my little specks seemed to come in\n",
      "a respect of any lunatic. He came in the train from Varna to Galatz.\n",
      "The blood was close in his hand. He had evidently not the other two were Exeter, and Dr. Van Helsing said:--\n",
      "\n",
      "\"But why do you know why, I have not forget these places again?\"\n",
      "\n",
      "\"My friend, it was best that we shall go in the track. I never taken out his hand and\n",
      "bringing his head in silence. After a while, and I can see nothing in\n",
      "a corner of the tide and the bolt ship. He replied:--\n",
      "\n",
      "\"As I tell you all this better distracted; and as you will come to to be off to get out of the room.\n",
      "\n",
      "\"We are the blind of work and the door behind them, and said in a sort of\n",
      "pall round her neck. The poor fellow is overway to the last horse of new which were more hard. But there was\n",
      "a sort of expreshering in such a corner of the game which had been said,\n",
      "with a very grave, sweet pride, and Arthur and I had been crying. He looked over them, and seemed to affect\n",
      "his intellect is same interfible. The man who are not all the conversation that he was\n",
      "inquietions of the railway by the Harker. The owner of the Professor in a sweet\n",
      "love and thinking of that time anyhow he is a beauty and the fast fit to discover of a dog, and then and\n",
      "there may be a closing down my head. He said this very hour, and then and there so small hole that only\n",
      "him as we were open fiercelan, for it was all so beautiful things. I am so glad that\n",
      "he has plenty of work to do when to the coffin. Arthur stepped\n",
      "forward and the love me to be kept with our terrible experiences in Thange from the\n",
      "same conscious of the facts and of the boxes at Carfax. We know from the road we were alone and had his\n",
      "presence on the face, as if time be lay away, and then to her I came away.\n",
      "\n",
      "I was not a spring, one land or with me, and of all things which they have some of the morning\n",
      "plecail the rest of us. We begin to help her, on the words of the pathway of the coffin in the trademark lines that we had seen him put\n",
      "into the room. I went to the window and called me to tell him that I am\n",
      "so glad I have taken the sleeping draught, and I cannot make at once. I must tell her that\n",
      "for some time, for I feared that the strain of the past the Borgo Pass. God grang the door be off, and we shall see.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER XXVI\n",
      "\n",
      "DR. SEWARD'S DIARY--_continued_\n",
      "\n",
      "\n",
      "It was just a quick blush on shorter. The service she told me that it was that followed came to me, and I could see his\n",
      "grave sayning, which was by the neck and the movement of his breast. It was\n",
      "now the snow flill and seemed to call out without\n",
      "the Harkers in the east of the same place, and came to any meaning about a week so\n",
      "what I might disturb her.\" The Professor said:--\n",
      "\n",
      "\"Well, guv'nor, you've treated a great fire of love--lether than a lovely\n",
      "woman. I wish I could get a strange thing which I had left them, and we\n",
      "shall all have to be lost. He can do that it for my late clear and the rest of us lay along the\n",
      "grave and horses. One thing and all the world that he can come with him\n",
      "in the mornin'. I wonder if there is no more to be delivered. In a fierce scare which was flapping its silent fingers and purchase of\n",
      "all the child which another old fellow, which was by the name of his time a most surching\n",
      "glass, he was like to think of him. This afternoon she was dazed for a minute, and then Quincey and\n",
      "how I saw him below me to his father's house, where they could have been destroyed them. It is well out of the\n",
      "stones are bitter respect. Then he went on, and very gravely and sounded by the conclusion that the hæmber had to\n",
      "see the ravings of the bands of pable and stayed with a little bit of perspiration\n",
      "and thought do who was became as if some time is with\n",
      "us.\"\n",
      "\n",
      "\"Dr. Van Helsing, that you are anxious of respectful spiritual into the hall; and to\n",
      "know the particular burden of silence. With a state of left\n",
      "way made be so far through the window. I am presty white flesh. Then he stopped\n",
      "with nurching from his face, and from what she had been for hands. Oh, I dare not the belief in that work,\n",
      "and my husband's lost yo\n"
     ]
    }
   ],
   "source": [
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "\n",
    "print(generate_text(model, 5000, .6, \"Dracula moved to Copenhagen because \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "id": "9q6NKUhHMERD",
    "outputId": "5058327b-2ed8-4561-d991-979885b4be06"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAJQCAYAAAAg+ngHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcnFWd7/Hvr5auql6qO72SlRDS\nBIEQEsIaQBAXFhlQcBA3hisiXlREnRnHuVfnOjNXHWdkRBTkKrK4oCKbyCIiyL4kQCAJSxLIvnSn\nO0nvS1Wd+0dVd0LopbpTTz1V3Z/3y3p1LU9XfuniMd/+nfOcY845AQAAwD8BvwsAAACY7AhkAAAA\nPiOQAQAA+IxABgAA4DMCGQAAgM8IZAAAAD4jkAEAAPiMQAYAAOAzAhkAAIDPQn4XMFa1tbVu9uzZ\nfpcBAAAwqmXLlu1wztWNdlzRBbLZs2dr6dKlfpcBAAAwKjNbn81xDFkCAAD4jEAGAADgMwIZAACA\nzwhkAAAAPiOQAQAA+IxABgAA4DMCGQAAgM8IZAAAAD4jkAEAAPiMQAYAAOAzAhkAAIDPCGQAAAA+\nI5ABAAD4jEAGAADgMwIZAACAzwhkAAAAPiOQAQAA+IxABgAA4DMCGQAAgM8IZAAAAD4jkAEAAPiM\nQAYAAOAzAtk+Vm9v13v+61E9uWaH36UAAIBJgkC2DyfpzeZO7ezq87sUAAAwSRDI9hEJpX8kvf0p\nnysBAACTBYFsH9FwUJLUk0j6XAkAAJgsCGT7iIYygYwOGQAAyBMC2T4i4fSPpKefDhkAAMgPAtk+\nBueQJeiQAQCA/CCQ7cPMFAkF1EuHDAAA5AmBbAjRcJAhSwAAkDcEsiFEQgGGLAEAQN4QyIZAhwwA\nAOQTgWwI0XCAZS8AAEDeEMiGEAkFWRgWAADkDYFsCNFwgK2TAABA3hDIhhAN0yEDAAD5QyAbQiTE\nHDIAAJA/BLIhRMJB9dIhAwAAeUIgG0I0FGQOGQAAyBsC2RDSy17QIQMAAPlBIBtCJBRkpX4AAJA3\nBLIh0CEDAAD5RCAbQjQcVCLllEjSJQMAAN4jkA0hEkr/WHoYtgQAAHlAIBtCNByUJPUybAkAAPKA\nQDaEaJgOGQAAyB8C2RAioXSHjIn9AAAgHwhkQxjokLE4LAAAyAcC2RAimTlkbDAOAADygUA2hChD\nlgAAII8IZEOIDAxZMqkfAADkAYFsCAMdMpa9AAAA+UAgG8LgshdM6gcAAHlAIBvC4KR+OmQAACAP\nCGRDiIaYQwYAAPKHQDaEKB0yAACQRwSyIQxuLs4cMgAAkAcEsiGEggGFAqZeFoYFAAB5QCAbRjQc\npEMGAADygkA2jGg4wNZJAAAgLwhkw4iEgkzqBwAAeUEgG0YkHGDZCwAAkBcEsmFEQ0G2TgIAAHlB\nIBtGJBxgUj8AAMgLAtkwoqEgy14AAIC8IJANI0qHDAAA5AmBbBjpdcjokAEAAO8RyIYRCXGVJQAA\nyA/PApmZRc3sOTNbbmYrzez/DHFMxMx+Y2ZrzOxZM5vtVT1jRYcMAADki5cdsl5J73HOLZB0lKQz\nzOz4fY75tKSdzrm5kq6W9F0P6xkTAhkAAMgXzwKZS+vIPAxnbm6fw86VdHPm/u2STjcz86qmsYiE\nAuphyBIAAOSBp3PIzCxoZi9JapL0kHPu2X0OmS5poyQ55xKSdkuq8bKmbEXCQfUlUnJu3wwJAACQ\nW54GMudc0jl3lKQZko41syPG8z5mdpmZLTWzpc3NzbktchjRcPpHw8R+AADgtbxcZemc2yXpEUln\n7PPSZkkzJcnMQpIqJbUM8f03OOcWO+cW19XVeV2upPTm4pKYRwYAADzn5VWWdWZWlbkfk/Q+Sa/t\nc9g9ki7O3L9A0l9cgYwR0iEDAAD5EvLwvadKutnMgkoHv9865+41s29JWuqcu0fSzyTdamZrJLVK\n+qiH9YxJlA4ZAADIE88CmXPuZUkLh3j+G3vd75H0Ea9q2B/R8EAgo0MGAAC8xUr9w4iE0j8aOmQA\nAMBrBLJhDHTImEMGAAC8RiAbxsCkfjpkAADAawSyYbDsBQAAyBcC2TBY9gIAAOQLgWwYe66ypEMG\nAAC8RSAbRmRgDhkdMgAA4DEC2TAG5pD10iEDAAAeI5ANgzlkAAAgXwhkwygJBmTGHDIAAOA9Atkw\nzEyRUIBABgAAPEcgG0E0HGTIEgAAeI5ANoJoKEiHDAAAeI5ANoJIOKCefjpkAADAWwSyEURDQfUm\n6JABAABvEchGEKVDBgAA8oBANoJImDlkAADAewSyEURCAbZOAgAAniOQjSAaDrJ1EgAA8ByBbASs\nQwYAAPKBQDYCVuoHAAD5QCAbQTQcoEMGAAA8RyAbASv1AwCAfCCQjSC9Un9Szjm/SwEAABMYgWwE\n0VBQKSclUgQyAADgHQLZCKLhoCQxbAkAADxFIBtBNJz+8bB9EgAA8BKBbASREB0yAADgPQLZCCKZ\nDhlLXwAAAC8RyEbAHDIAAJAPBLIRREIDHTICGQAA8A6BbAQDHbJeJvUDAAAPEchGMDhkSYcMAAB4\niEA2Apa9AAAA+UAgG8HAshfMIQMAAF4ikI2ADhkAAMgHAtkIoiwMCwAA8oBANoIIHTIAAJAHBLIR\nRJlDBgAA8oBANoJAwFQSDNAhAwAAniKQjSISCjCHDAAAeIpANopIOMjm4gAAwFMEslFEwwH10iED\nAAAeIpCNIhoOsnUSAADwFIFsFOk5ZAxZAgAA7xDIRhENB1n2AgAAeIpANopomA4ZAADwFoFsFJFQ\nkGUvAACApwhko4iGAyx7AQAAPEUgG0WUDhkAAPAYgWwUEeaQAQAAjxHIRhEJcZUlAADwFoFsFNFw\nUL10yAAAgIcIZKOIhgPqS6aUTDm/SwEAABMUgWwUkVBQkhi2BAAAniGQjSIaTv+IGLYEAABeIZCN\nIhpOd8jYYBwAAHiFQDaKSCj9I2LpCwAA4BUC2SgGOmTMIQMAAF4hkI1iYA4ZHTIAAOAVAtkoopmr\nLNk+CQAAeIVANorIYIeMQAYAALxBIBvFnnXIGLIEAADeIJCNYnDZCzpkAADAIwSyUQwse8HCsAAA\nwCsEslGw7AUAAPAagWwULHsBAAC8RiAbRYRlLwAAgMcIZKMIB00B4ypLAADgHQLZKMxM0XCQDhkA\nAPAMgSwL0XBQPUzqBwAAHiGQZSESCjCpHwAAeIZAloVoOMgcMgAA4BkCWRbSHTKGLAEAgDcIZFmI\nMKkfAAB4iECWhWgowJAlAADwDIEsC9FwUL10yAAAgEcIZFngKksAAOAlAlkW0ldZ0iEDAADeIJBl\nIRqmQwYAALxDIMsCK/UDAAAvEciywDpkAADASwSyLAys1O+c87sUAAAwARHIshANB+Wc1JdkHhkA\nAMg9AlkWIqH0j4mJ/QAAwAsEsixEwkFJYukLAADgCQJZFqKZDlkvHTIAAOABzwKZmc00s0fMbJWZ\nrTSzK4c45lQz221mL2Vu3/Cqnv0RzXTIuNISAAB4IeTheyckfcU594KZVUhaZmYPOedW7XPc4865\nD3pYx35jDhkAAPCSZx0y59xW59wLmfvtkl6VNN2rP89LUeaQAQAAD+VlDpmZzZa0UNKzQ7x8gpkt\nN7P7zezwfNQzVnuGLOmQAQCA3PNyyFKSZGblkn4v6UvOubZ9Xn5B0oHOuQ4zO0vSXZIah3iPyyRd\nJkmzZs3yuOJ32jNkSYcMAADknqcdMjMLKx3Gfumcu2Pf151zbc65jsz9+ySFzax2iONucM4tds4t\nrqur87LkIe0ZsqRDBgAAcs/LqyxN0s8kveqc+/4wxxyQOU5mdmymnhavahqvaJgOGQAA8I6XQ5ZL\nJH1S0itm9lLmua9LmiVJzrnrJV0g6XNmlpDULemjrgA3jIyEMnPImNQPAAA84Fkgc849IclGOeZa\nSdd6VUOuxDJDlt19BDIAAJB7rNSfhbJIOpB19CZ8rgQAAExEBLIshIIBlUdCausmkAEAgNwjkGUp\nHg2praff7zIAAMAERCDLUjwWVls3gQwAAOQegSxL8WiYDhkAAPAEgSxL8RhzyAAAgDcIZFmiQwYA\nALxCIMsSc8gAAIBXCGRZikdDau9NKJUquI0EAABAkSOQZSkeC8s5qZ3FYQEAQI4RyLIUj4YliWFL\nAACQcwSyLMVj6W0/mdgPAAByjUCWpXhsoEPGkCUAAMgtAlmWBocs6ZABAIAcI5BlqTLGHDIAAOAN\nAlmW9nTIGLIEAAC5RSDLUnk0M6mfDhkAAMgxAlmWggFTRSTEHDIAAJBzBLIxSG+fxJAlAADILQLZ\nGFRE6ZABAIDcI5CNARuMAwAALxDIxiAeDXOVJQAAyDkC2RjEYyE6ZAAAIOcIZGOQ7pARyAAAQG4R\nyMYgHgurvSehZMr5XQoAAJhACGRjEM8sDtvBPDIAAJBDBLIxGNzPkmFLAACQQwSyMYhnAtluJvYD\nAIAcIpCNwZ4NxglkAAAgdwhkYxCPDWwwzhwyAACQOwSyMaBDBgAAvEAgG4OBOWQsDgsAAHKJQDYG\nFZGQzMT2SQAAIKcIZGMQCJjKI2yfBAAAcotANkZsnwQAAHKNQDZG8ViYqywBAEBOEcjGKB4N0SED\nAAA5RSAbo3SHjEAGAAByh0A2RvEogQwAAOQWgWyM4rEQy14AAICcIpCNUWUsrI7ehBLJlN+lAACA\nCYJANkYD2yd19NIlAwAAuUEgG6M92ycRyAAAQG4QyMYoHg1JYoNxAACQOwSyMWKDcQAAkGsEsjEa\nmENGhwwAAOQKgWyM4rHMkCVzyAAAQI4QyMZocMiSDhkAAMgRAtkYlZeEZMYcMgAAkDsEsjEKBEwV\nEVbrBwAAuUMgGwc2GAcAALlEIBuHeDTMHDIAAJAzBLJxiMdCXGUJAAByhkA2DvFoWLsZsgQAADlC\nIBuHyhhDlgAAIHcIZOPApH4AAJBLBLJxiEfD6uxLKpFM+V0KAACYAAhk4zCwfVI7a5EBAIAcIJCN\nAxuMAwCAXCKQjcPgfpYsfQEAAHKAQDYO8Wh6yJIOGQAAyAUC2Tjs6ZARyAAAwP4jkI3DYCCjQwYA\nAHKAQDYOg0OWzCEDAAA5QCAbh7KSkAJGhwwAAOQGgWwcAgFTRZTV+gEAQG4QyMYpHgupjYVhAQBA\nDhDIxikeDWs3HTIAAJADBLJxqmSDcQAAkCMEsnGKR8NM6gcAADlBIBuneCzEshcAACAnCGTjRIcM\nAADkCoFsnOKxsLr6kupPpvwuBQAAFDkC2TgNrNbfztIXAABgPxHIxokNxgEAQK4QyMYpHmWDcQAA\nkBsEsnHa0yFjyBIAAOwfAtk4xWPpOWR0yAAAwP4ikI3T4JAlc8gAAMB+IpCN0+CQJR0yAACwnwhk\n41RWElTAmEMGAAD2H4FsnMxMU0pL1NLZ53cpAACgyBHI9kN9PKrm9h6/ywAAAEWOQLYf6isi2t7W\n63cZAACgyBHI9kNDPKImOmQAAGA/Ecj2Q0M8qub2XiVTzu9SAABAESOQ7Yf6iohSTmrpYNgSAACM\nn2eBzMxmmtkjZrbKzFaa2ZVDHGNmdo2ZrTGzl81skVf1eKE+HpUk5pEBAID9klUgM7ODzSySuX+q\nmX3RzKpG+baEpK845w6TdLykK8zssH2OOVNSY+Z2maTrxlS9zxoygYx5ZAAAYH9k2yH7vaSkmc2V\ndIOkmZJ+NdI3OOe2OudeyNxvl/SqpOn7HHaupFtc2jOSqsxs6lj+An5qiEck0SEDAAD7J9tAlnLO\nJSR9SNIPnXN/Lynr4GRmsyUtlPTsPi9Nl7Rxr8eb9M7QVrBqyyMyk7a30SEDAADjl20g6zeziyRd\nLOnezHPhbL7RzMqV7rB9yTnXNvYSJTO7zMyWmtnS5ubm8byFJ8LBgGrKStTUTocMAACMX7aB7BJJ\nJ0j6d+fcW2Z2kKRbR/smMwsrHcZ+6Zy7Y4hDNis9/DlgRua5t3HO3eCcW+ycW1xXV5dlyflRXxFV\nEx0yAACwH7IKZM65Vc65Lzrnfm1mUyRVOOe+O9L3mJlJ+pmkV51z3x/msHskfSpzteXxknY757aO\n5S/gt/p4RNuZ1A8AAPZDKJuDzOxRSX+TOX6ZpCYze9I59+URvm2JpE9KesXMXso893VJsyTJOXe9\npPsknSVpjaQupTtxRaWhIqqVW8Y1EgsAACApy0AmqdI512Zmlyp9VeQ3zezlkb7BOfeEJBvlGCfp\niixrKEgN8YhaOnqVSKYUCrLOLgAAGLtsE0QosxzF32rPpH4ovThsykktnX1+lwIAAIpUtoHsW5Ie\nlLTWOfe8mc2RtNq7sopHfcXAWmTMIwMAAOOT1ZClc+53kn631+M3JZ3vVVHFZHC1fhaHBQAA45Tt\n1kkzzOxOM2vK3H5vZjO8Lq4YDAQyrrQEAADjle2Q5c+VXqJiWub2h8xzk15teUlmtX46ZAAAYHyy\nDWR1zrmfO+cSmdtNkgprhVafhIIB1ZRFWBwWAACMW7aBrMXMPmFmwcztE5JavCysmDTEI2yfBAAA\nxi3bQPY/lF7yYpukrZIukPR3HtVUdBriUa6yBAAA45bt1knrnXN/45yrc87VO+fOE1dZDqqviDCH\nDAAAjNv+LC0/0rZJk0p9PKqWzvRq/QAAAGO1P4FsxG2RJpOGeETOSTs6WK0fAACM3f4EMpezKopc\nQ0VmLTLmkQEAgHEYcaV+M2vX0MHLJMU8qagI1cfZPgkAAIzfiIHMOVeRr0KK2eD2SSx9AQAAxmF/\nhiyRUVNWooCJxWEBAMC4EMhyIBQMqKacpS8AAMD4EMhypCEeYYNxAAAwLgSyHGmoiKqJDhkAABgH\nAlmO1MejaqJDBgAAxoFAliP1FRHt6OhTP6v1AwCAMSKQ5cjA0hc7Ohi2BAAAY0Mgy5GGwcVhCWQA\nAGBsCGQ5Us/2SQAAYJwIZDky0CFjcVgAADBWBLIcqSmPpFfrZ/skAAAwRgSyHAkGTHUVEYYsAQDA\nmBHIcqi+IsqkfgAAMGYEshxqiEcYsgQAAGNGIMuh+niUSf0AAGDMCGQ51FARVUtnn/oSrNYPAACy\nRyDLofrM0hfNrNYPAADGgECWQ6xFBgAAxoNAlkN7VuunQwYAALJHIMuhgSHLpnY6ZAAAIHsEshyq\nKYsoGDAWhwUAAGNCIMuhYMBUVx5RE0OWAABgDAhkOTatKqqNO7v8LgMAABQRAlmOza0v15qmTr/L\nAAAARYRAlmON9RXa0dGrnZ19fpcCAACKBIEsx+Y2lEuS1jR3+FwJAAAoFgSyHGusTwey1dsJZAAA\nIDsEshybVhlTaUlQq5va/S4FAAAUCQJZjgUClpnYT4cMAABkh0Dmgbn15QxZAgCArBHIPNBYX6Ft\nbT1q6+n3uxQAAFAECGQeGJjYz7AlAADIBoHMA40DS18wbAkAALJAIPPAjCmlioQCemM7V1oCAIDR\nEcg8EAyYDq4r12qGLAEAQBYIZB5pbGDpCwAAkB0CmUca68u1eVe3OnoTfpcCAAAKHIHMI3PrKyRJ\na+mSAQCAURDIPHJI5kpL5pEBAIDREMg8Mqu6VCXBAHtaAgCAURHIPBIKBjSnroy1yAAAwKgIZB6a\nW8/SFwAAYHQEMg811ldo484udfcl/S4FAAAUMAKZhxobyuWctLaZLhkAABgegcxDbDIOAACyQSDz\n0IE1ZQoFjCstAQDAiAhkHioJBTS7tkyrudISAACMgEDmscZ69rQEAAAjI5B5rLG+XOtaOtWb4EpL\nAAAwNAKZx+Y2VCjlpLd2dPpdCgAAKFAEMo8NXGnJPDIAADAcApnHDqotU8DYZBwAAAyPQOaxaDio\nA2vKtIalLwAAwDAIZHlwSEO5Vmxu87sMAABQoAhkeXD8nBptaO3SxtYuv0sBAAAFiECWByc31kqS\nnlyzw+dKAABAISKQ5cHBdeVqiEf0OIEMAAAMgUCWB2amJXNr9dSaHUqlnN/lAACAAkMgy5OTG2u1\ns6tfq7YyuR8AALwdgSxPlhycnkf2BMOWAABgHwSyPKmPRzWvoUJPrCaQAQCAtyOQ5dGSubV6bl2r\nevrZaBwAAOxBIMujkxtr1ZdIaem6nX6XAgAACgiBLI+OPahaoYAxjwwAALwNgSyPyiIhLZo1hQVi\nAQDA2xDI8uykxlqt2LJbOzv7/C4FAAAUCAJZni2ZWyvnpKfWtvhdCgAAKBAEsjxbMKNSFZGQnljT\n7HcpAACgQBDI8iwUDOj4g2uY2A8AAAYRyHxwcmOtNrZ2a31Lp9+lAACAAkAg88GSuWyjBAAA9iCQ\n+WBObZmmVkbZRgkAAEgikPnCzHRyY62eWLNDvQm2UQIAYLLzLJCZ2Y1m1mRmK4Z5/VQz221mL2Vu\n3/CqlkJ09pHT1N6T0F9ebfK7FAAA4DMvO2Q3STpjlGMed84dlbl9y8NaCs5Jc2vVEI/o9y9s8rsU\nAADgM88CmXPuMUmtXr1/sQsGTOctnK5HX2/Wjo5ev8sBAAA+8nsO2QlmttzM7jezw4c7yMwuM7Ol\nZra0uXniLKh6waIZSqSc7n5pi9+lAAAAH/kZyF6QdKBzboGkH0q6a7gDnXM3OOcWO+cW19XV5a1A\nrzU2VOjIGZX6/TKGLQEAmMx8C2TOuTbnXEfm/n2SwmZW61c9fjl/0Qyt2tqmV7e2+V0KAADwiW+B\nzMwOMDPL3D82U8uk23H7nAXTFA4aXTIAACYxL5e9+LWkpyXNM7NNZvZpM7vczC7PHHKBpBVmtlzS\nNZI+6pxzXtVTqKrLSnTavHrd9dIWJZIpv8sBAAA+CHn1xs65i0Z5/VpJ13r15xeT84+eoT+t2q7H\nV+/QaYfW+10OAADIM7+vsoSk0+bVa0ppWLezJhkAAJMSgawAlIQCOveo6Xpo1Xbt7ur3uxwAAJBn\nBLICcf6iGepLpHTvK6xJBgDAZEMgKxBHTI/rkIZyrrYEAGASIpAVCDPTR46eqRc27NILG3b6XQ4A\nAMgjAlkB+dhxs1RbXqLv3P+aJuEKIAAATFoEsgJSFgnpytMb9dxbrXrk9Sa/ywEAAHlCICswHz12\nlmbXlOq797+uZIouGQAAkwGBrMCEgwF99QPz9Pr2dt354ma/ywEAAHlAICtAZ8+fqgUzKvX9P72u\nnv6k3+UAAACPEcgKkJnpH888VFt29+jWp9f7XQ4AAPAYgaxAnXhwrd59SJ2ufWSNdnezej8AABMZ\ngayA/eMZh6qtp1/X/3Wt36UAAAAPEcgK2GHT4jrvqOm68Ym3tGlnl9/lAAAAjxDICtxXPzBPoYDp\nn+54hcViAQCYoAhkBW56VUxfO/NQPb56h363lH0uAQCYiAhkReDjxx2o4w6q1r/+cZW27e7xuxwA\nAJBjBLIiEAiYvnv+kepPpvTPdzJ0CQDAREMgKxKza8v01ffP08OvNenul7b4XQ4AAMghAlkRuWTJ\nQVo0q0r/8oeVam7v9bscAACQIwSyIhIMmP7jggXq6kvqG3ev8LscAACQIwSyIjO3vlxfem+j7l+x\nTXe8wFWXAABMBASyInTZyXN07EHV+vqdr+i1bW1+lwMAAPYTgawIhYIBXfuxhYpHw/rcL15QWw97\nXQIAUMwIZEWqviKqH318kTa0dunvf7ecpTAAAChiBLIidszsav3TmYfqwZXbdcNjb/pdDgAAGCcC\nWZH79EkH6ez5U/XdB17T02tb/C4HAACMA4GsyJmZvnvBkZpdW6Yv/PpFtlYCAKAIEcgmgPJISD/5\nxNHq7kvokpueZ5I/AABFhkA2QTQ2VOi6Txyt1dvb9dlblqk3kfS7JAAAkCUC2QRyyiF1+s+PLNDT\nb7boy79drlSKKy8BACgGIb8LQG6dt3C6mtt79e/3vaq68oi+ec5hMjO/ywIAACMgkE1Anzlljra3\n9einT7yluoqIrjhtrt8lAQCAERDIJqivn/UuNXf06nsPvq7a8hJdeMwsv0sCAADDIJBNUIGA6XsX\nLNDOrn597Y5XFAkFdd7C6X6XBQAAhsCk/gmsJBTQTz5xtI4/qEZf/u1L+uPLW/0uCQAADIFANsHF\nSoL66cWLtWjWFF1524t6aNV2v0sCAAD7IJBNAmWRkH5+yTE6fHqlrvjlC3r09Sa/SwIAAHshkE0S\nFdGwbrnkWM2tL9dnb12mJ9fs8LskAACQQSCbRCpLw/rFpcdpdk2ZLrnpef1p5Ta/SwIAACKQTTrV\nZSW67bLj9a6pcX3uly/o9mWb/C4JAIBJj0A2CU0pK9GvLj1Ox8+p1ld/t1w/ffxNv0sCAGBSI5BN\nUmWRkG78u2N05hEH6N/++Kr+88HX5Rx7XwIA4AcC2SQWCQV17ccW6cLFM3XtI2v0jbtXsiE5AAA+\nYKX+SS4YMH3n/PmqKg3rJ4+9qe7+pL57/pEKBtiQHACAfCGQQWamr515qEpLQrr6z2+opz+pqy88\nSuEgDVQAAPKBQAZJ6VB25XsbFQ0H9O37X1NvIqVrP7ZQkVDQ79IAAJjwaIHgbT777oP1rXMP10Or\ntuvSm5equy/pd0kAAEx4BDK8w6dOmK3/OP9IPbFmhy6+8Tnt7OzzuyQAACY0AhmG9LfHzNQ1H12o\nlzbt0rk/elJvbG/3uyQAACYsAhmGdc6CafrNZceruz+pD//4KT386na/SwIAYEIikGFEC2dN0T2f\nX6LZtaW69Jaluv6va1lAFgCAHCOQYVRTK2P63WdP1Nnzp+o797+mL/92uXr6mewPAECusOwFshIr\nCeqHFy3UvIYK/ddDb2h9S6d+8snFqquI+F0aAABFjw4ZsmZm+sLpjbru44u0amubzvvRk3p1a5vf\nZQEAUPQIZBizM+dP1e2Xn6hkyun8657SQ6uY7A8AwP4gkGFcjpheqbs/v0Rz68t12a1L9RMm+wMA\nMG4EMoxbQzyq31x2gs6aP1Xfvv81ff3OV5RIpvwuCwCAosOkfuyXWElQ1160UAfVlOnaR9Zoy64e\n/ejji1Qe4T8tAACyRYcM+83M9NUPzNN3PjxfT6zZoY9c/7S27e7xuywAAIoGgQw589FjZ+nGvztG\nG1o69aEfcwUmAADZIpAhp959SJ1+d/mJck76yPVP64EV2/wuCQCAgkcgQ84dNi2uO684UXPqynT5\nL5bpG3evYGV/AABGQCCDJ6bGtSE+AAAaCklEQVRWxnT75SfqMycfpFueXq8P/fgprW3u8LssAAAK\nEoEMnikJBfTPZx+mG/9usbbt7tY5P3xCv1+2ye+yAAAoOAQyeO49hzbo/itP0fzplfrK75brH29/\nWb0JhjABABhAIENeHFAZ1a8+c7w+f9pc/WbpRl34k2dYGgMAgAwCGfImGEivV3b9JxZp9fZ2ffCH\nT+j5da1+lwUAgO8IZMi7M46YqruuWKKKaEgX3fCMbn1mPftgAgAmNQIZfNHYUKG7rliikxtr9b/v\nWqGv/G65OnsTfpcFAIAvCGTwTWUsrJ9dfIyuPL1Rd764Wedc+4RWbWF1fwDA5EMgg68CAdNV7ztE\nv7r0eHX0JHTej5/ULU+vYwgTADCpEMhQEE44uEb3X3myTjy4Rt+4e6Uu/8Uy7e7q97ssAADygkCG\nglFTHtGNFx+jfz7rXXr41Saddc3jenHDTr/LAgDAcwQyFJRAwPSZU+bo9s+dKLP0BuU/ffxNhjAB\nABMagQwF6aiZVfrjF0/W6e+q17/98VV95pal2tXV53dZAAB4gkCGglUZC+v6Txytb55zmP76RrPO\nvuYJLVvPECYAYOIhkKGgmZkuWXKQfv+5ExUMmC78ydO66cm3GMIEAEwoBDIUhSNnVOkPXzhJp86r\n17/8YZW+eNtLLCQLAJgwCGQoGpWxsG745NH6hzPm6Y8vb9F5P3pSa5o6/C4LAID9RiBDUQkETP/z\n1Lm69dPHqbWzT+de+4TufXmL32UBALBfCGQoSkvm1ureL56keQdU6PO/elFX3vYiV2ECAIoWgQxF\na2plTL/57Am66r2H6I8vb9X7rn5MD7+63e+yAAAYMwIZilo4GNCV723UXVcsUU1ZiT5981J95bfL\ntbubbZcAAMWDQIYJ4Yjplbrn8yfp86fN1V0vbdYZ//2Ynlq7w++yAADICoEME0ZJKKCvfmCe7vjc\niYqFg/r4T5/Vt+97VX2JlN+lAQAwIgIZJpwFM6t07xdP0kXHztJPHntTH/oxy2MAAAqbZ4HMzG40\nsyYzWzHM62Zm15jZGjN72cwWeVULJp/SkpD+74fm64ZPHq0tu7r1wR8+rlufWc8K/wCAguRlh+wm\nSWeM8PqZkhozt8skXedhLZik3n/4AXrwS6fomNnV+t93rdCnbnxOm3d1+10WAABv41kgc849Jql1\nhEPOlXSLS3tGUpWZTfWqHkxe9fGobr7kWP3ruYdr2fqd+sDVj+m25zbQLQMAFAw/55BNl7Rxr8eb\nMs+9g5ldZmZLzWxpc3NzXorDxBIImD55wmw9+KVTdMT0uL52xyu6+OfPawvdMgBAASiKSf3OuRuc\nc4udc4vr6ur8LgdFbGZ1qX516fH613MP19J1rfrA1Y/pF8+sVypFtwwA4B8/A9lmSTP3ejwj8xzg\nqb27ZfNnVOp/3bVCF97wNFdiAgB842cgu0fSpzJXWx4vabdzbquP9WCSmVldql9eepy+d8GRemN7\nh876weP6wZ9Xs24ZACDvQl69sZn9WtKpkmrNbJOkb0oKS5Jz7npJ90k6S9IaSV2SLvGqFmA4ZqaP\nLJ6pU+fV61v3rtLVf35D9768Rd+94EgtmjXF7/IAAJOEFduVZosXL3ZLly71uwxMUI+81qR/vvMV\nbWvr0WdOnqOr3neIouGg32UBAIqUmS1zzi0e7biimNQP5Mtph9brwatO0YXHzNRPHntTZ1/zuF7Y\nsNPvsgAAExyBDNhHRTSsb3/4SN3yP45Vd19SF1z3lL5936vq6U/6XRoAYIIikAHDOOWQOj1w1Sn6\n28Xpbtmp33tUv3hmPZP+AQA5RyADRhCPhvWd84/UbZcdr+lTYvpfd63Qe/7rUf126UYlkgQzAEBu\nEMiALBw/p0a3X36CbrrkGFWXlegfbn9Z77v6Mf1p5Ta/SwMATAAEMiBLZqZT59Xr7iuW6IZPHq2S\nYECX3bpMV/3mJe3u6ve7PABAESOQAWNkZnr/4Qfo3i+epCtPb9Q9y7foA//9mP76BvusAgDGh0AG\njFM4GNBV7ztEd/3PJaqIhnTxjc/p63e+oo7ehN+lAQCKDIEM2E/zZ1TqD184SZedMke/fm6D3vOf\nj+qmJ99imQwAQNYIZEAORMNBff2sd+n2y0/Q7Noy/csfVund33tENz+1jmAGABgVWycBOeac09Nr\nW3T1n9/Q8+t26oB4VFe8Z64uXDxTJSF+BwKAySTbrZMIZIBHnHN6ck06mC1bv1Mzq2P68vsO0d8s\nmK5gwPwuDwCQB+xlCfjMzHRSY61uv/wE/fySY1QRCeuq3yzXWT94XA+t2q5i+2UIAOAdAhngMTPT\nafPqde8XTtIPL1qovmRKn7llqc6/7iktXdfqd3kAgAJAIAPyJBAwnbNgmv501Sn69ofna/Oubl1w\n/dP67K1Ltba5w+/yAAA+Yg4Z4JOuvoRufOItXffoWvUkUvrYsbN05XsbVVse8bs0AECOMKkfKBI7\nOnr1gz+v1q+e26BoKKCLT5ytS0+eo+qyEr9LAwDsJwIZUGTWNnfo+w+9ofte2arScFCfOnG2PkMw\nA4CiRiADitTq7e265i9rdO/LWxQLB/XJEw7UZ06ew1AmABQhAhlQ5NY0teuHf1mje5ZvUSQU0EeP\nmaXLTpmjaVUxv0sDAGSJQAZMEGubO3T9o2t154ubZSZ9eOEMfe7UgzW7tszv0gAAoyCQARPMpp1d\nuuGxN3Xb8xuVSKZ0+rsa9PHjZumUxjoFWPkfAAoSgQyYoJrae3TTk+v0m+c3qqWzTzOrY/rYsQfq\nI4tnMM8MAAoMgQyY4HoTSf1p5Xb98tn1eubNVoWDptMPbdB5C6fptEPrFQkF/S4RACa9bANZKB/F\nAMi9SCiocxZM0zkLpmlNU7t+/dxG3f3SFj2wcpvi0ZDOmj9V5x41XccdVM2QJgAUODpkwASSSKb0\n1NoW3fXiZj2wcpu6+pKaWR3TRcfO0keOnqm6CoY0ASCfGLIEJrmuvoQeWrVdtz23UU+/2aJw0PT+\nww7Qx4+bpRMOrpEZXTMA8BqBDMCgtc0d+vWzG3T7C5u0q6tfc+rKdMmSg3T+oukqLWHmAgB4hUAG\n4B16+pO6f8VW3fTkOi3ftFuVsbAuOnaWLj7xQE2tZMFZAMg1AhmAYTnntGz9Tv3sibf04MptMjO9\n/7AGnTl/qk6bV6eKaNjvEgFgQuAqSwDDMjMtnl2txbOrtbG1Szc/tU53vbRF96/YppJgQEvm1uiM\nIw7Qe9/VoBrWNgMAz9EhAyBJSqacXtiwUw+u2KYHVm7Tpp3dCgZMJ8yp0dlHTtUHDj9A1WUlfpcJ\nAEWFIUsA4+ac06qtbbrvla2675VtemtHp4IB04kHpztnxx1UrTm15axvBgCjIJAByIm9w9kfX96q\ndS1dkqSKaEhHzazSwllTtGhWlU44uIbdAQBgHwQyADnnnNObOzr1wvqdenHjLr24YZde39amlJMq\nY2Gds2CqPrRwhhbNqmKdMwAQgQxAnnT2JvTcW62666XNenDlNvX0pzS7plQfWjhDp7+rXodNjTO0\nCWDSIpAByLv2nn7dv2Kb7nhhk555s1WSVFUa1nEHVevEg2t14sE1mltfTvcMwKRBIAPgq+1tPXp6\nbYueWrtDT65p0eZd3ZKkhnhEpzTW6d3z6nTS3FpVlXLlJoCJi0AGoKBsbO3Sk2t26PHVO/T46ma1\n9SQUMGnBzCotnDlFB9WWanZtmQ6qLdO0yhjDnAAmBAIZgIKVSKa0fNNu/fWNZj2+ulmvbW1Xd39y\n8PWSUECN9eU6+sApg7fpVTGGOgEUHQIZgKLhnNP2tl69taNT61o69daOTq3YvFsvbdylrr50UGuI\nR3T0gVO0aNYULZ5drcOnxRUOBnyuHABGxtZJAIqGmemAyqgOqIzqhINrBp9PJFN6bVu7XtiwU0vX\n7dSy9Tt13yvbJEnRcEBHzazSsbOrddqh9Vowo4phTgBFiw4ZgKKybXePlq3fqaXrW7Vs/U6t2Lxb\nKSfVlpfo1Hn1Ov3Qep3UWMsG6QAKAkOWACaFXV19+usbzXr41SY9+nqT2noSkqS6ioimVUY1rSqm\naVUxTa+Kaf6MSh0xrVKxEnYUAJAfBDIAk04imdKy9Tv1zJut2rKrW1t2d6e/7uoZvGggFDC9a2pc\nC2dV6aiZVTpsWlxzastVEmI+GoDcI5ABQIZzTs0dvXp54269uHGnXtywS8s37lJn5oKBcNB0cF25\n5h1QoXkHVGh6VUy15RHVlkdUU16iKaUlCjI/DcA4EMgAYATJlNOapg69tq1Nr21r12tb2/T6tnZt\n2d3zjmMDJtVXRDV9yp7hz+lTYprXUKGjZlbRXQMwLK6yBIARBAM22BE7d6/n23v6tb2tRzs6+rSj\no1ctma9bd/do885uLd+4Sw+s2Kr+ZPqX2Wg4oMUHVuuEg2t0/JwaHTmjkuU4AIwZgQwA9lIRDasi\nGtbc+uGPSaacmtp79PKm3Xp6bYueebNF33vwdUnpOWozq0t1YE2pZteUaXZNqWZMKVVlaViVsbDi\n0bDisZBi4SAL3QJ5MjBtYc32Dq1u6tCapg6tbmrXqfPqdfm7D/a7PEkEMgAYs2DANLUypqmVMX3g\n8AMkSS0dvXr2rVat2Lxb61u6tK6lU8+/1To4T21f4aCpqrRE1aUlmlIW1pTSEk0pK1FdeUR1FXvd\nMnPZuDIUSAerLbt79Mb2dq3e3q63dnSprbtfbT39autJqL2nXx09CSVTTknnlEo5pZzUn0ypN5Ea\nfJ+KaEiN9eUqLaDzikAGADlQUx7RWfOn6qz5Uwefc85pR0eftuzqVltPv3Z396utO6G2nn7t6urX\nrq4+7ezq087Ofq1p6lBrZ59au/o01NTeWDiomvIS1ZSVqLqsRFWlJaqIhjK3dOetuiycCXFR1VUQ\n4lBcUimn/lRKiaRTW0+/NrZ2a0Nrlza0dmlja5fe2tGpNU0d6uhNDH5PTVmJqkrDmXMgpBlVMZVH\nQgoFTcGAKWDpWzAgTa+KqbGhQnPry1VfESm4DjWBDAA8YmaDna5sJZIptXb2qam9V80dvWpu61VL\nZ59aOnrV2tmnHZnX1jR3qL0nobbufqWGuTarPBJSZSysskhQsZKQSsNBlUWCKo+EVJO5grSmrEQ1\nZRFVlw9060oUj4YK7h8rFJae/qRWbmnTixvSizP3JlIKBkzhYEChgCkUDKirL6Gde/3isauzP92l\nMskkmUkmk5NTf9IpOcx/yGbStMqYZlWX6sOLpquxoULzGip0SEO5qkpL8vsX9xCBDAAKSCgYUH08\nqvp4NKvjnXPq6kuqradfrZ19am7vVXN7bzrQtfeqrbtfXX1JdfUn1d2X0JZd/YPHdg0znBoKpIdT\n49GQUm7PP5aJlJNzTmWZoBePpb9WxsKqLY+oviKiuoqoGuIR1cejCgdNclLKSSnn5JQeqi0tSc+h\nG2opkURmaCkUNEVCdPiGkkw59SaS6u1P/6x6E8n0zyxgmc8l/I4LS7r7kmrt6lNrRzoctWeG9wa+\ntvWkO7dtmS7u7u5+dfYlVFYSUjwWysytDCkSCuj1be1atbVt8MKWqZVRVURDSiT3dLj6k06lJUFN\nKU0Px8+pLVNVaYmi4aCcnDL/k3NOZqZw0BQKBFQSSge6skhIs6pLNbO6VNOrYpPiSmYCGQAUMbP0\nP15lkZCmVsbG9L3dfUm1dKavJG3p7NXOzv70EGpXn1o7+9Xe069gID30Ew4EFAyaTFJHb7ozt7u7\nX9vberWrq1+tnb3DduqGEwkFVFqSvrihtz8dKhKZNzGTpsajmlVTqgOryzSrplQHxKMy09uGdM2k\n0pJMB7AkqNKSYPoffZcOkIm9wuQ7pecXJVNuz81lkoKUDg4Z5ZGwZkyJqSEefUeQTKactuzq1sbW\nLm1v71FPf0o9/cnBr6lMiC2PpIeYyyPpQNqTSKqzN6muvoQ6epPq3Ovnuqt7YIg7Hai7+5Pq6U+H\nsL5kSqMpLQmqMhZWwEwtnb3q6R/+e8zS3dR4NDwYtA+sKVVZJKTO3oTaexJqau/RmqaEuvqSmltf\npk+fNEcLZ1Vp4cyqrH95wMgIZAAwScVKgppRkr4KdH8lU04tHb3a3tarpvYeNbf3qj/lFDApYDY4\nRNWXdOruS//D3t2XHOzSRUIBRcIBRUJBRUIBdfUltaG1S+tbOvXwa9u1o6Nvv2vMhVDANLUqqhlV\npQoFTRtbu7R5V/dgt2go+4bIkQwEqYFO18zqUpVlQuaeW0DRcPrnNPDzKgkFlEiltLsr3e3anQl0\nzmlwcePqsrCqyyKZOVehwXlXZSUhBVj42HcEMgDAfgsGbK+h1sqcv39Hb0ItHb2Dj9MRT0o6p+6+\npLr70yGvszep3kRSATOFMt29UDA9sXuoeXEBS9cetMwk8MxE8D1/Trph1tbdr007u7VpZ5c27ezW\n5l3d6uxL6fDplTpz/lQdWF2qWdWlaqiMprt0oeBgaDKTevpTau9NXwHY0ZtQZ29SsZKgyiNBlZak\nQ1FpJMgadpMYgQwAUPDKM0N+xSpWElSsJKj6Cr8rQaEiigMAAPiMQAYAAOAzAhkAAIDPCGQAAAA+\nI5ABAAD4jEAGAADgMwIZAACAzwhkAAAAPiOQAQAA+IxABgAA4DMCGQAAgM8IZAAAAD4jkAEAAPiM\nQAYAAOAzAhkAAIDPCGQAAAA+I5ABAAD4jEAGAADgMwIZAACAzwhkAAAAPiOQAQAA+IxABgAA4DMC\nGQAAgM/MOed3DWNiZs2S1ufhj6qVtCMPfw7Ghs+lcPHZFCY+l8LE51K4cv3ZHOicqxvtoKILZPli\nZkudc4v9rgNvx+dSuPhsChOfS2Hicylcfn02DFkCAAD4jEAGAADgMwLZ8G7wuwAMic+lcPHZFCY+\nl8LE51K4fPlsmEMGAADgMzpkAAAAPiOQ7cPMzjCz181sjZl9ze96Jiszm2lmj5jZKjNbaWZXZp6v\nNrOHzGx15usUv2udrMwsaGYvmtm9mccHmdmzmXPnN2ZW4neNk42ZVZnZ7Wb2mpm9amYncM4UBjO7\nKvP/ZSvM7NdmFuWc8YeZ3WhmTWa2Yq/nhjxPLO2azGf0spkt8qouAtlezCwo6UeSzpR0mKSLzOww\nf6uatBKSvuKcO0zS8ZKuyHwWX5P0sHOuUdLDmcfwx5WSXt3r8XclXe2cmytpp6RP+1LV5PYDSQ84\n5w6VtEDpz4dzxmdmNl3SFyUtds4dISko6aPinPHLTZLO2Oe54c6TMyU1Zm6XSbrOq6IIZG93rKQ1\nzrk3nXN9km6TdK7PNU1KzrmtzrkXMvfblf6HZbrSn8fNmcNulnSePxVObmY2Q9LZkn6aeWyS3iPp\n9swhfDZ5ZmaVkk6R9DNJcs71Oed2iXOmUIQkxcwsJKlU0lZxzvjCOfeYpNZ9nh7uPDlX0i0u7RlJ\nVWY21Yu6CGRvN13Sxr0eb8o8Bx+Z2WxJCyU9K6nBObc189I2SQ0+lTXZ/bekf5CUyjyukbTLOZfI\nPObcyb+DJDVL+nlmKPmnZlYmzhnfOec2S/pPSRuUDmK7JS0T50whGe48yVsuIJChoJlZuaTfS/qS\nc65t79dc+hJhLhPOMzP7oKQm59wyv2vB24QkLZJ0nXNuoaRO7TM8yTnjj8x8pHOVDs3TJJXpnUNm\nKBB+nScEsrfbLGnmXo9nZJ6DD8wsrHQY+6Vz7o7M09sH2sWZr01+1TeJLZH0N2a2Tulh/fcoPXep\nKjMcI3Hu+GGTpE3OuWczj29XOqBxzvjvvZLecs41O+f6Jd2h9HnEOVM4hjtP8pYLCGRv97ykxsyV\nLyVKT7q8x+eaJqXMnKSfSXrVOff9vV66R9LFmfsXS7o737VNds65f3LOzXDOzVb6HPmLc+7jkh6R\ndEHmMD6bPHPObZO00czmZZ46XdIqcc4Ugg2Sjjez0sz/tw18NpwzhWO48+QeSZ/KXG15vKTdew1t\n5hQLw+7DzM5Sen5MUNKNzrl/97mkScnMTpL0uKRXtGee0teVnkf2W0mzJK2X9LfOuX0nZyJPzOxU\nSV91zn3QzOYo3TGrlvSipE8453r9rG+yMbOjlL7QokTSm5IuUfoXb84Zn5nZ/5F0odJXkL8o6VKl\n5yJxzuSZmf1a0qmSaiVtl/RNSXdpiPMkE6CvVXqIuUvSJc65pZ7URSADAADwF0OWAAAAPiOQAQAA\n+IxABgAA4DMCGQAAgM8IZAAAAD4jkAEoemaWNLOX9rrlbANtM5ttZity9X4AMJTQ6IcAQMHrds4d\n5XcRADBedMgATFhmts7M/sPMXjGz58xsbub52Wb2FzN72cweNrNZmecbzOxOM1ueuZ2Yeaugmf0/\nM1tpZn8ys1jm+C+a2arM+9zm018TwARAIAMwEcT2GbK8cK/Xdjvn5iu92vZ/Z577oaSbnXNHSvql\npGsyz18j6a/OuQVK7wO5MvN8o6QfOecOl7RL0vmZ578maWHmfS736i8HYOJjpX4ARc/MOpxz5UM8\nv07Se5xzb2Y2q9/mnKsxsx2Spjrn+jPPb3XO1ZpZs6QZe29fY2azJT3knGvMPP5HSWHn3L+Z2QOS\nOpTeduUu51yHx39VABMUHTIAE50b5v5Y7L2/YFJ75t+eLelHSnfTnjcz5uUCGBcCGYCJ7sK9vj6d\nuf+UpI9m7n9c6Y3sJelhSZ+TJDMLmlnlcG9qZgFJM51zj0j6R0mVkt7RpQOAbPDbHICJIGZmL+31\n+AHn3MDSF1PM7GWlu1wXZZ77gqSfm9nfS2qWdEnm+Ssl3WBmn1a6E/Y5SVuH+TODkn6RCW0m6Rrn\n3K6c/Y0ATCrMIQMwYWXmkC12zu3wuxYAGAlDlgAAAD6jQwYAAOAzOmQAAAA+I5ABAAD4jEAGAADg\nMwIZAACAzwhkAAAAPiOQAQAA+Oz/A3U5tCuiMJUeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "loss = history.history['loss']\n",
    "newLoss = np.squeeze(loss)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(newLoss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Generation with RNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
